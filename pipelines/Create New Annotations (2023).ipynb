{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e368b1",
   "metadata": {},
   "source": [
    "# Generate annotations of movie plots using Stanford coreNLP 4.5.5\n",
    "\n",
    "1. Download Stanford CoreNLP 4.5.5 from [here](https://nlp.stanford.edu/software/stanford-corenlp-4.5.5.zip) (you will Java 8 installed and added to PATH)\n",
    "2. Launch a server from terminal: `java -Xmx24g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -annotators tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment -coref.algorithm neural -parse.originalDependencies true -outputFormat text` \n",
    "\n",
    "`-Xmx24g` for allocating heap space up to 24GB, `-timeout 1200000` for a server timeout of 20 minutes, `-annotators tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment` for the Stanford annotators we are interested in, `-coref.algorithm neural` for the neural network version of the coreference algorithm, `-parse.originalDependencies true` to use the original dependency names which will match the ones of 2013. **Adjust the memory parameter according to your machine**.\n",
    "\n",
    "3. Run the `Step 1` cell\n",
    "4. If some files didn't pass or exhausted memory, reboot and run the `Step 2`, if some files still persist, use the terminal and run the Stanford coreNLP pipeline directly on those specific files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61280e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from helpers.readers import read_dataframe\n",
    "\n",
    "PATH_OUT = '../nlp_results/'\n",
    "if not os.path.exists(PATH_OUT):\n",
    "    os.makedirs(PATH_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries = read_dataframe('cmu/summaries', usecols=[\n",
    "    \"Wikipedia movie ID\", \n",
    "    \"Plot Summary\"\n",
    "])\n",
    "\n",
    "# removing special character\n",
    "plot_summaries['Plot Summary'] = plot_summaries['Plot Summary'].str.replace('\\\\\\\\', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104eceb",
   "metadata": {},
   "source": [
    "### Step 1: Parallel Run (adjust workers if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c396de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# server propertiers (the request can modify the server properties)\n",
    "url = \"http://localhost:9000\"\n",
    "properties = {\n",
    "    \"annotators\": \"tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment\",\n",
    "    \"coref.algorithm\": \"neural\",\n",
    "    \"parse.originalDependencies\": \"true\",\n",
    "    \"outputFormat\": \"text\",\n",
    "}\n",
    "\n",
    "def process_summary(row):\n",
    "    data = row['Plot Summary']\n",
    "    response = requests.post(url, params={\"properties\": str(properties)}, data=data.encode('utf-8'))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        output_file = os.path.join(PATH_OUT, f'nlp_movie_{row[\"Wikipedia movie id\"]}.txt')\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            outfile.write(response.text.strip())\n",
    "    else:\n",
    "        print(f\"Error processing movie ID {row['Wikipedia movie id']}: {response.status_code}\")\n",
    "\n",
    "# Adjust max_workers according to your machine\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    future_to_row = {executor.submit(process_summary, row): row for index, row in plot_summaries.iterrows()}\n",
    "    for future in tqdm(as_completed(future_to_row), total=len(future_to_row)):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd2e3a",
   "metadata": {},
   "source": [
    "Some files didn't went through also got some out of memory issue around 30k (can happen earlier based on machine CPU RAM, nb workers, allowed memory for server). Solution is to save the id of the files that didn't went through, reboot, and restart the annotation for error ids and non annotated files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2370e99",
   "metadata": {},
   "source": [
    "### Step 2: Run again\n",
    "Find how many files are not annotated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d77a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42301"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_ids_from_filenames(folder_path):\n",
    "    ids = []\n",
    "    pattern = r'nlp_movie_(\\d+).txt'\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            ids.append(int(match.group(1))) \n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "folder_path = '../nlp_results/'\n",
    "annoated_ids = extract_ids_from_filenames(folder_path)\n",
    "len(annoated_ids) # missing 2 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef5151",
   "metadata": {},
   "source": [
    "Launch server from terminal: `java -Xmx24g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -annotators tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment -coref.algorithm neural -parse.originalDependencies true -outputFormat text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db33685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "plot_summaries = read_dataframe('cmu/summaries', usecols=[\n",
    "    \"Wikipedia movie ID\", \n",
    "    \"Plot Summary\"\n",
    "])\n",
    "\n",
    "filtered_summaries = plot_summaries[\n",
    "    (~plot_summaries['Wikipedia movie id'].isin(annoated_ids)) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce959d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia movie id</th>\n",
       "      <th>plot_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29218</th>\n",
       "      <td>16019180</td>\n",
       "      <td>The film shows a selection of Suras from the Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34186</th>\n",
       "      <td>30039</td>\n",
       "      <td>The film begins with a prologue, the only comm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Wikipedia movie id                                       plot_summary\n",
       "29218            16019180  The film shows a selection of Suras from the Q...\n",
       "34186               30039  The film begins with a prologue, the only comm..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e5526",
   "metadata": {},
   "source": [
    "### Parallel Run (adjust workers if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a77c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:9000\"\n",
    "properties = {\n",
    "    \"annotators\": \"tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment\",\n",
    "    \"coref.algorithm\": \"neural\",\n",
    "    \"parse.originalDependencies\": \"true\",\n",
    "    \"outputFormat\": \"text\",\n",
    "}\n",
    "\n",
    "def process_summary(row):\n",
    "    data = row['plot_summary']\n",
    "    response = requests.post(url, params={\"properties\": str(properties)}, data=data.encode('utf-8'))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        output_file = os.path.join(PATH_OUT, f'nlp_movie_{row[\"Wikipedia movie id\"]}.txt')\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            outfile.write(response.text.strip())\n",
    "    else:\n",
    "        print(f\"Error processing movie ID {row['Wikipedia movie id']}: {response.status_code}\")\n",
    "\n",
    "# Adjust max_workers according to your machine\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    future_to_row = {executor.submit(process_summary, row): row for index, row in filtered_summaries.iterrows()}\n",
    "    for future in tqdm(as_completed(future_to_row), total=len(future_to_row)):\n",
    "        future.result() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2732e86",
   "metadata": {},
   "source": [
    "### Step 3: Manual Stanford pipeline for naughty files\n",
    "\n",
    "Example: `java -Xmx16g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment -coref.algorithm neural -parse.originalDependencies true -outputFormat text -file 16019180.txt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
