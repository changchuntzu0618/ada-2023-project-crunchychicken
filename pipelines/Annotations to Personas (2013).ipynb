{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('..')\n",
    "from helpers.readers import read_dataframe\n",
    "\n",
    "DATA_PATH = '../generated/annotations_2013/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1829337",
   "metadata": {},
   "source": [
    "### How to get the data\n",
    "1. Download from [here](https://drive.google.com/drive/folders/1qEoAM8HcLksss1-gfKOTN_6GDi6-MsTT?usp=sharing) `tokens.parquet` and `dependencies.parquet` (generated from [`XML to Dataframes (2013).ipnyb`](https://github.com/epfl-ada/ada-2023-project-crunchychicken/blob/main/pipelines/XML%20to%20Dataframes%20(2013).ipynb))\n",
    "2. Place the files inside `annotations_2013/`:\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── P2.ipynb\n",
    "│\n",
    "├── generated/\n",
    "│   ├── annotations_2013/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85045515",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = read_dataframe('cmu/tokens_2013')\n",
    "dependencies = read_dataframe('cmu/dependencies_2013')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c2882",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Find movies that contain at least one character (from tokens check for atleast one `NER==\"PERSON\"`) and check that dependencies contain only the dependencies we are looking for as in *Learning Latent Personas of Film Characters* from David Bamman Brendan O’Connor Noah A. Smith.\n",
    "\n",
    "![dep_needed](https://i.postimg.cc/sfbxBMV6/image-2023-11-15-113351627.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34686580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[tokens[\"NER\"] == \"PERSON\"].head(3) # using old annotations, NER can miss some characters, for example label them as ORGANIZATION\n",
    "movies_with_atleast_one_person = tokens[tokens[\"NER\"] == \"PERSON\"][\"movie_id\"].unique().tolist()\n",
    "len(movies_with_atleast_one_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ad268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_verbs = [\"agent\", \"nsubj\"]\n",
    "patient_verbs = [\"dobj\", \"nsubjpass\", \"iobj\"] # + prep_* (handled seperatly)\n",
    "attributes_av = [\"nsubj\", \"appos\"]\n",
    "attributes_pv = [\"nsubj\", \"appos\", \"amod\", \"nn\"]\n",
    "\n",
    "dependencies_needed = dependencies[(dependencies[\"dependency_class\"] == \"collapsed-ccprocessed\") & \n",
    "                                   ((dependencies[\"dependency_type\"].isin(agent_verbs + patient_verbs + attributes_av + attributes_pv)) |  \n",
    "                                    (dependencies[\"dependency_type\"].str.startswith(\"prep_\")))].copy()\n",
    "\n",
    "dependencies_needed.head(3)\n",
    "\n",
    "movies_with_atleast_one_dep = dependencies_needed[\"movie_id\"].unique().tolist()\n",
    "\n",
    "len(movies_with_atleast_one_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "set_movies_with_person = set(movies_with_atleast_one_person)\n",
    "set_movies_with_dep = set(movies_with_atleast_one_dep)\n",
    "\n",
    "common_movies = set_movies_with_person.intersection(set_movies_with_dep)\n",
    "\n",
    "common_movies_list = list(common_movies)\n",
    "\n",
    "len(common_movies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = tokens[tokens[\"movie_id\"].isin(common_movies_list)].copy()\n",
    "filtered_dependencies = dependencies_needed[dependencies_needed[\"movie_id\"].isin(common_movies_list)].copy()\n",
    "\n",
    "filtered_tokens = filtered_tokens.drop([\"COB\", \"COE\"], axis=1)\n",
    "filtered_dependencies = filtered_dependencies.drop([\"dependency_class\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge1 = pd.merge(filtered_tokens, filtered_dependencies,\n",
    "                 left_on=['movie_id', 'sentence_id', 'token_id'],\n",
    "                 right_on=['movie_id', 'sentence_id', 'governor_id'],\n",
    "                 how='inner')\n",
    "\n",
    "merge2 = pd.merge(filtered_tokens, filtered_dependencies,\n",
    "                 left_on=['movie_id', 'sentence_id', 'token_id'],\n",
    "                 right_on=['movie_id', 'sentence_id', 'dependent_id'],\n",
    "                 how='inner')\n",
    "\n",
    "merged_data = pd.concat([merge1, merge2]).drop_duplicates().reset_index(drop=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1097a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[(merged_data[\"token_id\"] != merged_data[\"governor_id\"]) & (merged_data[\"token_id\"] != merged_data[\"dependent_id\"])] # should be empty ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install memory_profiler\n",
    "\n",
    "# install and load memory_profiler to use %memit, use %whos to see what's in memory\n",
    "%load_ext memory_profiler \n",
    "%memit\n",
    "import gc\n",
    "\n",
    "del common_movies\n",
    "del set_movies_with_dep\n",
    "del set_movies_with_person\n",
    "del common_movies_list\n",
    "del movies_with_atleast_one_dep\n",
    "del movies_with_atleast_one_person\n",
    "\n",
    "del merge1\n",
    "del merge2\n",
    "\n",
    "del filtered_tokens\n",
    "del filtered_dependencies\n",
    "\n",
    "del dependencies_needed\n",
    "del dependencies\n",
    "del tokens\n",
    "\n",
    "gc.collect()\n",
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf179a",
   "metadata": {},
   "source": [
    "### For each plot:\n",
    "1. Extract the characters (NER=PERSON)\n",
    "2. For each character extract (dependencies + coreference)\n",
    "    - agent verbs\n",
    "    - patient verbs\n",
    "    - attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71773e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.DataFrame(columns=['movie_id', 'character', 'AV', 'PV', 'Att']) # dataframe where we will store the character, its actions (agent and patient) and attributes\n",
    "verb_pos_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"] # to indentify verbs from attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7992b",
   "metadata": {},
   "source": [
    "### Extractor using parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# there are more precise ways to do it, but as we are using not very accurate annotations data, we will spend more time enhancing the new annotations.\n",
    "\n",
    "def process_group(chunk):\n",
    "    temp_data = []\n",
    "    for (movie_id, character), group in chunk.groupby(['movie_id', 'word']):\n",
    "        AV, PV, Att = [], [], [] \n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            if row['governor_word'] == character:\n",
    "                dependency_word = row['dependent_word']\n",
    "                dependency_idx = row['dependent_id']\n",
    "            elif row['dependent_word'] == character:\n",
    "                dependency_word = row['governor_word']\n",
    "                dependency_idx = row['governor_id']\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "\n",
    "            pos_row = merged_data[(merged_data['movie_id'] == movie_id) & \n",
    "                                  (merged_data['sentence_id'] == row['sentence_id']) & \n",
    "                                  (merged_data['token_id'] == dependency_idx)]\n",
    "\n",
    "            if not pos_row.empty and pos_row.iloc[0]['POS'] in verb_pos_tags:\n",
    "                if row['dependency_type'] in agent_verbs or row['dependency_type'].startswith(\"prep_\"):\n",
    "                    AV.append(pos_row.iloc[0]['lemma'])\n",
    "                elif row['dependency_type'] in patient_verbs:\n",
    "                    PV.append(pos_row.iloc[0]['lemma'])\n",
    "            else:\n",
    "                Att.append(pos_row.iloc[0]['lemma'])\n",
    "\n",
    "        temp_data.append({\n",
    "            'movie_id': movie_id,\n",
    "            'character': character,\n",
    "            'AV': AV,\n",
    "            'PV': PV,\n",
    "            'Att': Att\n",
    "        })\n",
    "    return temp_data\n",
    "\n",
    "character_data = merged_data[merged_data[\"NER\"] == \"PERSON\"]\n",
    "\n",
    "num_partitions = joblib.cpu_count() \n",
    "chunk_size = int(character_data.shape[0] / num_partitions)\n",
    "chunks = [character_data.iloc[i:i + chunk_size] for i in range(0, character_data.shape[0], chunk_size)]\n",
    "\n",
    "results = Parallel(n_jobs=num_partitions)(delayed(process_group)(chunk) for chunk in tqdm(chunks))\n",
    "\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "characters = pd.concat([pd.DataFrame(flattened_results)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab952fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.to_parquet(os.path.join(DATA_PATH, \"characters.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters # PV looks empty but it's not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb138724",
   "metadata": {},
   "source": [
    "### Generate bag of words\n",
    "A bag is a tupple of $(r,w)$, where $r$ is of {agent verb, patient verb, attribute} and $w$ is the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bags_of_words(characters_df: pd.DataFrame):\n",
    "    bags_of_words = []\n",
    "\n",
    "    for _, row in tqdm(characters_df.iterrows()):\n",
    "        movie_id = row['movie_id']\n",
    "        character_name = row['character']\n",
    "\n",
    "        av = row['AV'] if isinstance(row['AV'], list) else []\n",
    "        pv = row['PV'] if isinstance(row['PV'], list) else []\n",
    "        att = row['Att'] if isinstance(row['Att'], list) else []\n",
    "\n",
    "        for verb in av:\n",
    "            bags_of_words.append((movie_id, character_name, 'agent_verb', verb))\n",
    "\n",
    "        for verb in pv:\n",
    "            bags_of_words.append((movie_id, character_name, 'patient_verb', verb))\n",
    "\n",
    "        for attribute in att:\n",
    "            bags_of_words.append((movie_id, character_name, 'attribute', attribute))\n",
    "\n",
    "    return bags_of_words\n",
    "\n",
    "bags_of_words = generate_bags_of_words(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_df = pd.DataFrame(bags_of_words, columns=['movie_id', 'character_name', 'type', 'word'])\n",
    "bags_df.to_parquet(os.path.join(DATA_PATH, \"bags.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214f6db",
   "metadata": {},
   "source": [
    "### From tupples to topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb930aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_df = read_dataframe('cmu/bags_2013')\n",
    "bags_of_words=bags_df.values\n",
    "\n",
    "bags_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb705da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently global character version (doesn't make the distinction of the same character in different movies)\n",
    "character_docs = defaultdict(list)\n",
    "for _, character, _, word in bags_of_words:\n",
    "    character_docs[character].append(word)\n",
    "    \n",
    "for character in character_docs:\n",
    "    character_docs[character] = \" \".join(character_docs[character])\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(character_docs.values())\n",
    "\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, verbose=2, max_iter=10, random_state = 0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lda, os.path.join(DATA_PATH, \"lda_model.gz\"), compress=('gzip', 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = joblib.load(os.path.join(DATA_PATH, \"lda_model.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff637bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic {topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "n_topic_words = 10\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), n_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a72c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_topic = lda.transform(X)\n",
    "character_names = list(character_docs.keys())\n",
    "\n",
    "character_classification=[]\n",
    "\n",
    "for i, topic_dist in enumerate(character_topic):\n",
    "    topic_most_prob = topic_dist.argmax()\n",
    "    character_classification.append((character_names[i], topic_most_prob, topic_dist))\n",
    "\n",
    "character_classification_df=pd.DataFrame(character_classification,columns=['character_name', 'topic', 'topic_dist'])\n",
    "character_classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_classification_df.to_parquet(os.path.join(DATA_PATH, \"character_classification.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa86635",
   "metadata": {},
   "outputs": [],
   "source": [
    "character='Batman'\n",
    "\n",
    "character_topics = character_classification_df[character_classification_df['character_name'] == character]['topic_dist'].iloc[0]\n",
    "\n",
    "topics = range(len(character_topics))\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topics, character_topics)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.title(f'Topic Distribution for {character}')\n",
    "plt.yscale('log')  \n",
    "plt.xticks(topics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = character_classification_df['topic'].value_counts()\n",
    "\n",
    "topic_counts = topic_counts.sort_index()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topic_counts.index, topic_counts.values)\n",
    "\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Characters')\n",
    "plt.title('Distribution of Characters Across Topics')\n",
    "plt.xticks(topic_counts.index)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
