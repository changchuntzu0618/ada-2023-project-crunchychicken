{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e526a162",
   "metadata": {},
   "source": [
    "# Parse the generated new annotations\n",
    "The new annotations were generated from [Create New Annotations (2023).ipnyb](https://github.com/epfl-ada/ada-2023-project-crunchychicken/blob/main/pipelines/Create%20New%20Annotations%20(2023).ipynb).\n",
    "\n",
    "The files are opened one by one and read line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea53d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PATH_OUT = '../generated/annotations_2023/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e0247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_info(line):\n",
    "    match = re.match(r\"Sentence #(\\d+) \\((\\d+) tokens, sentiment: (.+?)\\):\", line)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2)), match.group(3)\n",
    "    return None, None, None\n",
    "\n",
    "def extract_token_info(line):\n",
    "    token_pattern = r\"\\[Text=(.*?) CharacterOffsetBegin=(\\d+) CharacterOffsetEnd=(\\d+) PartOfSpeech=(.*?) Lemma=(.*?) NamedEntityTag=(.*?) SentimentClass=(.*?)\\]\"\n",
    "    match = re.match(token_pattern, line)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None, None, None, None, None, None, None\n",
    "\n",
    "def extract_dependency_info(line):\n",
    "    dep_type = line.split(\"(\", 1)[0]\n",
    "    reverse_1 = line.split(\"(\", 1)[1].split(\" \", 1)[0][::-1]\n",
    "    reverse_2 = line.split(\"(\", 1)[1].split(\" \", 1)[1][::-1]\n",
    "    word_1 = reverse_1[1:].split(\"-\", 1)[1][::-1]\n",
    "    word_1_idx = reverse_1[1:].split(\"-\", 1)[0][::-1]\n",
    "    word_2 = reverse_2[1:].split(\"-\", 1)[1][::-1]\n",
    "    word_2_idx = reverse_2[1:].split(\"-\", 1)[0][::-1]    \n",
    "    return dep_type, word_1.strip(), int(word_1_idx), word_2.strip(), int(word_2_idx)\n",
    "\n",
    "def extract_entity_mentions_info(line):\n",
    "    entity_pattern = r\"([^\\t]+)\\t([^\\t]+)\\t(?:[^\\t:]+:)?(-?\\d*\\.\\d+|-)\"\n",
    "    match = re.match(entity_pattern, line)\n",
    "    if match:\n",
    "        word, word_type, probability = match.groups()\n",
    "        probability = None if probability == '-' else float(probability)\n",
    "        return word, word_type, probability\n",
    "    return None, None, None\n",
    "\n",
    "def extract_coreference_info(line):\n",
    "    coords_1 = line.split(\", that is: \")[0].split(\" -> \")[0]\n",
    "    coords_2 = line.split(\", that is: \")[0].split(\" -> \")[1]\n",
    "    word_1 = line.split(\", that is: \")[1].split(\" -> \")[0][1:-1]\n",
    "    word_2 = line.split(\", that is: \")[1].split(\" -> \")[1][1:-1]\n",
    "    return coords_1, coords_2, word_1, word_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fdbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    movie_id = re.search(r\"nlp_movie_(\\d+).txt\", file_path).group(1)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        current_section = None\n",
    "        prev_section = None\n",
    "        parse_data = ''\n",
    "        data = {\n",
    "            'sentences': [],\n",
    "            'tokens': [],\n",
    "            'constituency_parse': [],\n",
    "            'binary_parse': [],\n",
    "            'sentiment_tree': [],\n",
    "            'dependencies': [],\n",
    "            'entity_mentions': [],\n",
    "            'coreferences': []\n",
    "        }\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"Sentence #\"):\n",
    "                current_section = 'sentence'\n",
    "                sentence_id, tokens, sentiment = extract_sentence_info(line)\n",
    "                data['sentences'].append((int(movie_id), int(sentence_id), int(tokens), sentiment))\n",
    "            elif line.startswith(\"Tokens\"):\n",
    "                current_section = 'tokens'\n",
    "            elif line.startswith(\"Constituency parse\"):\n",
    "                current_section = 'constituency_parse'\n",
    "            elif line.startswith(\"Binary Constituency parse\"):\n",
    "                current_section = 'binary_parse'\n",
    "            elif line.startswith(\"Sentiment-annotated binary tree\"):\n",
    "                current_section = 'sentiment_tree'\n",
    "            elif line.startswith(\"Dependency Parse (enhanced plus plus dependencies):\"):\n",
    "                current_section = 'dependencies'\n",
    "            elif line.startswith(\"Extracted the following NER entity mentions:\"):\n",
    "                current_section = 'entity_mentions'\n",
    "            elif line.startswith(\"Coreference set:\"):\n",
    "                current_section = 'coreferences'\n",
    "            else:\n",
    "                if current_section == 'tokens' and line:\n",
    "                    word, cob, coe, pos, lemma, ner, sentiment = extract_token_info(line)\n",
    "                    data['tokens'].append((int(movie_id), int(sentence_id), word, int(cob), int(coe), pos, lemma, ner, sentiment))\n",
    "                    prev_section = 'tokens'\n",
    "                elif current_section == 'constituency_parse' and line:\n",
    "                    parse_data += line + ' '\n",
    "                    prev_section = 'constituency_parse'\n",
    "                elif current_section == 'binary_parse' and line:\n",
    "                    if prev_section == 'constituency_parse':\n",
    "                        data['constituency_parse'].append((int(movie_id), int(sentence_id), parse_data.strip()))\n",
    "                        parse_data = ''\n",
    "                    parse_data += line + ' '\n",
    "                    prev_section = 'binary_parse'\n",
    "                elif current_section == 'sentiment_tree' and line:\n",
    "                    if prev_section == 'binary_parse':\n",
    "                        data['binary_parse'].append((int(movie_id), int(sentence_id), parse_data.strip()))\n",
    "                        parse_data = ''\n",
    "                    parse_data += line + ' '\n",
    "                    prev_section = 'sentiment_tree'\n",
    "                elif current_section == 'dependencies' and line:\n",
    "                    if prev_section == 'sentiment_tree':\n",
    "                        data['sentiment_tree'].append((int(movie_id), int(sentence_id), parse_data.strip()))\n",
    "                        parse_data = ''\n",
    "                    dep_type, word_1, word_1_idx, word_2, word_2_idx = extract_dependency_info(line)\n",
    "                    if dep_type is None:\n",
    "                        print(f\"None dependency found: movie_id {movie_id}, sentence_id: {setence_id}\")\n",
    "                        print(line)\n",
    "                    data['dependencies'].append((int(movie_id), int(sentence_id), dep_type, word_1, int(word_1_idx), word_2, int(word_2_idx)))\n",
    "                    prev_section = 'dependencies'\n",
    "                elif current_section == 'entity_mentions' and line:\n",
    "                    word, entity_type, optional_probability = extract_entity_mentions_info(line)\n",
    "                    if word is None:\n",
    "                        print(f\"None entity found: movie_id {movie_id}, sentence_id: {setence_id}\")\n",
    "                        print(line)\n",
    "                    data['entity_mentions'].append((int(movie_id), int(sentence_id), word, entity_type, optional_probability))\n",
    "                    prev_section = 'entity_mentions'\n",
    "                elif current_section == 'coreferences' and line:\n",
    "                    coords_1, coords_2, word_1, word_2 = extract_coreference_info(line)\n",
    "                    data['coreferences'].append((int(movie_id), coords_1, coords_2, word_1, word_2))\n",
    "                    prev_section = 'coreferences'\n",
    "\n",
    "        return data\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Cannot read file {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95c0f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66487b16e23b47648edec15e35e032f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files:   0%|          | 0/42303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = \"../nlp_results/\"\n",
    "nb_workers = 16\n",
    "\n",
    "columns_sentences = ['Wikipedia_movie_id', 'Sentence_id', 'Nb_tokens', 'Sentiment']\n",
    "df_sentences = pd.DataFrame(columns=columns_sentences)\n",
    "columns_tokens = [\"Wikipedia_movie_id\", \"Sentence_id\", \"Word\", \"COB\", \"COE\", \"POS\", \"Lemma\",\"NER\",\"Sentiment\"]\n",
    "df_tokens = pd.DataFrame(columns=columns_tokens)\n",
    "columns_constituency_parse = [\"Wikipedia_movie_id\", \"Sentence_id\", \"Constituency_parse\"]\n",
    "df_constituency_parse = pd.DataFrame(columns=columns_constituency_parse)\n",
    "columns_binary_parse = [\"Wikipedia_movie_id\", \"Sentence_id\", \"Binary_parse\"]\n",
    "df_binary_parse = pd.DataFrame(columns=columns_binary_parse)\n",
    "columns_sentiment_tree = [\"Wikipedia_movie_id\", \"Sentence_id\", \"Sentiment_tree\"]\n",
    "df_sentiment_tree = pd.DataFrame(columns=columns_sentiment_tree)\n",
    "columns_dependencies = [\"Wikipedia_movie_id\", \"Sentence_id\", \"Dependency_type\", \"Word_1\", \"Word_1_idx\", \"Word_2\", \"Word_2_idx\"]\n",
    "df_dependencies = pd.DataFrame(columns=columns_dependencies) \n",
    "columns_entities = [\"Wikipedia_movie_id\", \"Sentence_id\",\"Word\",\"Entity_type\",\"Optional_probability\"]\n",
    "df_entities = pd.DataFrame(columns=columns_entities)\n",
    "columns_coreference = [\"Wikipedia_movie_id\", \"Coordinates_1\", \"Coordinates_2\", \"Word_1\",\"Word_2\"]\n",
    "df_coreference = pd.DataFrame(columns=columns_coreference)\n",
    "\n",
    "\n",
    "def handle_file(filename):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.txt'):\n",
    "        #if \"nlp_movie_32179375\" in filename: # for debug\n",
    "        return process_file(file_path)\n",
    "    return None\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=nb_workers) as executor:\n",
    "    future_to_file = {executor.submit(handle_file, filename): filename for filename in os.listdir(folder_path)}\n",
    "    progress = tqdm(total=len(future_to_file), desc=\"Processing Files\")\n",
    "    for future in as_completed(future_to_file):\n",
    "        data = future.result()\n",
    "        if data:\n",
    "            df_sentences = pd.concat([df_sentences, pd.DataFrame(data['sentences'], columns=columns_sentences)])\n",
    "            df_tokens = pd.concat([df_tokens, pd.DataFrame(data['tokens'], columns=columns_tokens)])\n",
    "            df_constituency_parse = pd.concat([df_constituency_parse, pd.DataFrame(data['constituency_parse'], columns=columns_constituency_parse)])\n",
    "            df_binary_parse = pd.concat([df_binary_parse, pd.DataFrame(data['binary_parse'], columns=columns_binary_parse)])\n",
    "            df_sentiment_tree = pd.concat([df_sentiment_tree, pd.DataFrame(data['sentiment_tree'], columns=columns_sentiment_tree)])\n",
    "            df_dependencies = pd.concat([df_dependencies, pd.DataFrame(data['dependencies'], columns=columns_dependencies)])\n",
    "            df_entities = pd.concat([df_entities, pd.DataFrame(data['entity_mentions'], columns=columns_entities)])\n",
    "            df_coreference = pd.concat([df_coreference, pd.DataFrame(data['coreferences'], columns=columns_coreference)])\n",
    "        progress.update(1)\n",
    "\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd064d95",
   "metadata": {},
   "source": [
    "18h45 for tokens and dependencies (parallel bug?); 2h20 sentences, entities, coref; 40mins consitutuency and binary parses sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77728942",
   "metadata": {},
   "source": [
    "### Checking created dataframes and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a079dcf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 662514 entries, 0 to 34\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Wikipedia_movie_id  662514 non-null  int32 \n",
      " 1   Sentence_id         662514 non-null  int16 \n",
      " 2   Nb_tokens           662514 non-null  int16 \n",
      " 3   Sentiment           662514 non-null  string\n",
      "dtypes: int16(2), int32(1), string(1)\n",
      "memory usage: 15.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_sentences.isna().any()\n",
    "df_sentences['Wikipedia_movie_id'] = pd.to_numeric(df_sentences['Wikipedia_movie_id'], downcast='integer')\n",
    "df_sentences['Sentence_id'] = pd.to_numeric(df_sentences['Sentence_id'], downcast='integer')\n",
    "df_sentences['Nb_tokens'] = pd.to_numeric(df_sentences['Nb_tokens'], downcast='integer')\n",
    "df_sentences['Sentiment'] = df_sentences['Sentiment'].astype('string')\n",
    "df_sentences.info()\n",
    "df_sentences.to_parquet(os.path.join(PATH_OUT, \"sentences.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185563fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15046378"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tokens)\n",
    "df_tokens.isna().any()\n",
    "\n",
    "df_tokens['Wikipedia_movie_id'] = pd.to_numeric(df_tokens['Wikipedia_movie_id'], downcast='integer')\n",
    "df_tokens['Sentence_id'] = pd.to_numeric(df_tokens['Sentence_id'], downcast='integer')\n",
    "df_tokens['COB'] = pd.to_numeric(df_tokens['COB'], downcast='integer')\n",
    "df_tokens['COE'] = pd.to_numeric(df_tokens['COE'], downcast='integer')\n",
    "columns_to_convert = ['Word', 'POS', 'Lemma', 'NER', 'Sentiment']\n",
    "df_tokens[columns_to_convert] = df_tokens[columns_to_convert].astype('string')\n",
    "df_tokens.info()\n",
    "df_tokens.to_parquet(os.path.join(PATH_OUT, \"tokens.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dddcd472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 662514 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Wikipedia_movie_id  662514 non-null  int32 \n",
      " 1   Sentence_id         662514 non-null  int16 \n",
      " 2   Constituency_parse  662514 non-null  string\n",
      "dtypes: int16(1), int32(1), string(1)\n",
      "memory usage: 13.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_constituency_parse['Wikipedia_movie_id'] = pd.to_numeric(df_constituency_parse['Wikipedia_movie_id'], downcast='integer')\n",
    "df_constituency_parse['Sentence_id'] = pd.to_numeric(df_constituency_parse['Sentence_id'], downcast='integer')\n",
    "df_constituency_parse['Constituency_parse'] = df_constituency_parse['Constituency_parse'].astype('string')\n",
    "df_constituency_parse.info()\n",
    "df_constituency_parse.to_parquet(os.path.join(PATH_OUT, \"constituency_parse.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae34dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 662514 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Wikipedia_movie_id  662514 non-null  int32 \n",
      " 1   Sentence_id         662514 non-null  int16 \n",
      " 2   Binary_parse        662514 non-null  string\n",
      "dtypes: int16(1), int32(1), string(1)\n",
      "memory usage: 13.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_binary_parse['Wikipedia_movie_id'] = pd.to_numeric(df_binary_parse['Wikipedia_movie_id'], downcast='integer')\n",
    "df_binary_parse['Sentence_id'] = pd.to_numeric(df_binary_parse['Sentence_id'], downcast='integer')\n",
    "df_binary_parse['Binary_parse'] = df_binary_parse['Binary_parse'].astype('string')\n",
    "df_binary_parse.info()\n",
    "df_binary_parse.to_parquet(os.path.join(PATH_OUT, \"binary_parse.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ce1d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 662514 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Wikipedia_movie_id  662514 non-null  int32 \n",
      " 1   Sentence_id         662514 non-null  int16 \n",
      " 2   Sentiment_tree      662514 non-null  string\n",
      "dtypes: int16(1), int32(1), string(1)\n",
      "memory usage: 13.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_sentiment_tree['Wikipedia_movie_id'] = pd.to_numeric(df_sentiment_tree['Wikipedia_movie_id'], downcast='integer')\n",
    "df_sentiment_tree['Sentence_id'] = pd.to_numeric(df_sentiment_tree['Sentence_id'], downcast='integer')\n",
    "df_sentiment_tree['Sentiment_tree'] = df_sentiment_tree['Sentiment_tree'].astype('string')\n",
    "df_sentiment_tree.info()\n",
    "df_sentiment_tree.to_parquet(os.path.join(PATH_OUT, \"sentiment_tree.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "804e940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependencies['Wikipedia_movie_id'] = pd.to_numeric(df_dependencies['Wikipedia_movie_id'], downcast='integer')\n",
    "df_dependencies['Sentence_id'] = pd.to_numeric(df_dependencies['Sentence_id'], downcast='integer')\n",
    "df_dependencies['Word_1_idx'] = pd.to_numeric(df_dependencies['Word_1_idx'], downcast='integer')\n",
    "df_dependencies['Word_2_idx'] = pd.to_numeric(df_dependencies['Word_2_idx'], downcast='integer')\n",
    "columns_to_convert = ['Dependency_type', 'Word_1', 'Word_2']\n",
    "df_dependencies[columns_to_convert] = df_dependencies[columns_to_convert].astype('string')\n",
    "df_dependencies.info()\n",
    "df_dependencies.to_parquet(os.path.join(PATH_OUT, \"dependencies.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4806de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2220379 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   Wikipedia_movie_id    int32  \n",
      " 1   Sentence_id           int16  \n",
      " 2   Word                  string \n",
      " 3   Entity_type           string \n",
      " 4   Optional_probability  float32\n",
      "dtypes: float32(1), int16(1), int32(1), string(2)\n",
      "memory usage: 72.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_entities.isna().any()\n",
    "df_entities['Wikipedia_movie_id'] = pd.to_numeric(df_entities['Wikipedia_movie_id'], downcast='integer')\n",
    "df_entities['Sentence_id'] = pd.to_numeric(df_entities['Sentence_id'], downcast='integer')\n",
    "df_entities['Optional_probability'] = pd.to_numeric(df_entities['Optional_probability'], downcast='float')\n",
    "df_entities['Word'] = df_entities['Word'].astype('string')\n",
    "df_entities['Entity_type'] = df_entities['Entity_type'].astype('string')\n",
    "df_entities.info()\n",
    "df_entities.to_parquet(os.path.join(PATH_OUT, \"entities.parquet\"), compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56998044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1879672 entries, 0 to 76\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   Wikipedia_movie_id  int32 \n",
      " 1   Sentence_id         int16 \n",
      " 2   Coordinates_1       string\n",
      " 3   Coordinates_2       string\n",
      " 4   Word_1              string\n",
      " 5   Word_2              string\n",
      "dtypes: int16(1), int32(1), string(4)\n",
      "memory usage: 82.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_coreference.isna().any()\n",
    "df_coreference['Wikipedia_movie_id'] = pd.to_numeric(df_coreference['Wikipedia_movie_id'], downcast='integer')\n",
    "columns_to_convert = ['Coordinates_1', 'Coordinates_2', 'Word_1', 'Word_2']\n",
    "df_coreference[columns_to_convert] = df_coreference[columns_to_convert].astype('string')\n",
    "df_coreference.info()\n",
    "df_coreference.to_parquet(os.path.join(PATH_OUT, \"coreference.parquet\"), compression='brotli')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
