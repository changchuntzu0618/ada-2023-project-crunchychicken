{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027bcc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mutliple outputs in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# cell width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f74e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "PATH_IN = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63eaae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLP data\n",
    "tokens_fname = os.path.join(PATH_IN, 'df_tokens.parquet')\n",
    "dependencies_fname = os.path.join(PATH_IN, 'df_dependencies.parquet')\n",
    "coref_fname = os.path.join(PATH_IN, 'df_coreference.parquet')\n",
    "entities_fname= os.path.join(PATH_IN, 'df_entities.parquet')\n",
    "\n",
    "tokens = pd.read_parquet(tokens_fname)\n",
    "dependencies = pd.read_parquet(dependencies_fname)\n",
    "coreference = pd.read_parquet(coref_fname)\n",
    "entities = pd.read_parquet(entities_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e60bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['token_id'] = tokens.groupby(['Wikipedia_movie_id', 'Sentence_id']).cumcount() + 1 # adding token id, not present by default in 4.5.5\n",
    "tokens = tokens.drop(['Sentiment', 'COB', 'COE'], axis=1) # can use Sentiment later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336cb08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15046378, 15376339, 1879672, 2220379)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15M            15M               1.8M               2.2M\n",
    "len(tokens), len(dependencies), len(coreference), len(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c5a42",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Running on such big dataframes will take too much time. To solve this, we will first filter on movie plots that contain a character (from the entities dataframe check for `Entity_type == PERSON` and a non NaN `Optional_probability`) and on also filter on the dependencies (from the dependencies dataframe check that the `Dependency_type` satisfies an agent verb, patient verb or attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c69e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.7 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent_verbs = [\"agent\", \"nsubj\"]\n",
    "patient_verbs = [\"dobj\", \"nsubjpass\", \"iobj\"] # no prep_ using coreNLP4.5.5\n",
    "attributes_av = [\"nsubj\", \"appos\"]\n",
    "attributes_pv = [\"nsubj\", \"appos\", \"amod\", \"nn\"]\n",
    "\n",
    "def is_matching_dependency(dep_type):\n",
    "    if dep_type in agent_verbs or dep_type in patient_verbs or dep_type in attributes_av or dep_type in attributes_pv:\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "tokens_with_character = tokens[tokens[\"NER\"] == \"PERSON\"][\"Wikipedia_movie_id\"].tolist()\n",
    "entities_with_character = entities[(entities['Entity_type'] == 'PERSON') & (entities['Optional_probability'].notna())][\"Wikipedia_movie_id\"].tolist()\n",
    "prefiltered_entities = entities[(entities['Entity_type'] == 'PERSON') & (entities['Optional_probability'].notna())].copy()\n",
    "dependencies_with_dep = dependencies[dependencies['Dependency_type'].apply(is_matching_dependency)][\"Wikipedia_movie_id\"].tolist()\n",
    "prefiltered_dependencies = dependencies[dependencies['Dependency_type'].apply(is_matching_dependency)].copy()\n",
    "\n",
    "set_tokens = set(tokens_with_character)\n",
    "set_entities = set(entities_with_character)\n",
    "set_dependencies = set(dependencies_with_dep)\n",
    "\n",
    "intersection_set = set_tokens & set_entities & set_dependencies\n",
    "\n",
    "filtered_tokens = tokens[tokens[\"Wikipedia_movie_id\"].isin(intersection_set)]\n",
    "filtered_entities = prefiltered_entities[prefiltered_entities[\"Wikipedia_movie_id\"].isin(intersection_set)]\n",
    "filtered_dependencies = prefiltered_dependencies[prefiltered_dependencies[\"Wikipedia_movie_id\"].isin(intersection_set)]\n",
    "filtered_coreference = coreference[coreference[\"Wikipedia_movie_id\"].isin(intersection_set)] # complex to use, maybe for P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e81993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14714763, 3603836, 1856122, 848088)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tokens), len(filtered_dependencies), len(filtered_coreference), len(filtered_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7077d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 49s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_to_character = {}\n",
    "for _, row in filtered_entities.iterrows():\n",
    "    character_name = row['Word']\n",
    "    movie_id = row['Wikipedia_movie_id']\n",
    "    sentence_id = row['Sentence_id']\n",
    "    for word in character_name.split():\n",
    "        key = (movie_id, sentence_id, word)\n",
    "        word_to_character[key] = character_name\n",
    "\n",
    "filtered_tokens_copy = filtered_tokens.copy()\n",
    "\n",
    "def map_word_to_character(row):\n",
    "    key = (row['Wikipedia_movie_id'], row['Sentence_id'], row['Word'])\n",
    "    return word_to_character.get(key, '')\n",
    "\n",
    "filtered_tokens_copy['Character'] = filtered_tokens_copy.apply(map_word_to_character, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e899fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.1 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge1 = pd.merge(filtered_tokens_copy, filtered_dependencies, left_on=['Wikipedia_movie_id', 'Sentence_id', 'token_id'], right_on=['Wikipedia_movie_id', 'Sentence_id', 'Word_1_idx'])\n",
    "\n",
    "merge2 = pd.merge(filtered_tokens_copy, filtered_dependencies, left_on=['Wikipedia_movie_id', 'Sentence_id', 'token_id'], right_on=['Wikipedia_movie_id', 'Sentence_id', 'Word_2_idx'])\n",
    "\n",
    "final_merged_df = pd.concat([merge1, merge2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0295d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_id</th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>NER</th>\n",
       "      <th>token_id</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dependency_type</th>\n",
       "      <th>Word_1</th>\n",
       "      <th>Word_1_idx</th>\n",
       "      <th>Word_2</th>\n",
       "      <th>Word_2_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>nn</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>10</td>\n",
       "      <td>George Sanders</td>\n",
       "      <td>nn</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>10</td>\n",
       "      <td>George</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>couple</td>\n",
       "      <td>NN</td>\n",
       "      <td>couple</td>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>couple</td>\n",
       "      <td>15</td>\n",
       "      <td>Joyces</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>couple</td>\n",
       "      <td>NN</td>\n",
       "      <td>couple</td>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>amod</td>\n",
       "      <td>couple</td>\n",
       "      <td>15</td>\n",
       "      <td>English</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>gone</td>\n",
       "      <td>VBN</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>gone</td>\n",
       "      <td>18</td>\n",
       "      <td>who</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207667</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>min</td>\n",
       "      <td>NNP</td>\n",
       "      <td>min</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>asks</td>\n",
       "      <td>12</td>\n",
       "      <td>min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207668</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>min</td>\n",
       "      <td>NNP</td>\n",
       "      <td>min</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>get</td>\n",
       "      <td>14</td>\n",
       "      <td>min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207669</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>head</td>\n",
       "      <td>NN</td>\n",
       "      <td>head</td>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>nn</td>\n",
       "      <td>surgery</td>\n",
       "      <td>22</td>\n",
       "      <td>head</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207670</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>toe</td>\n",
       "      <td>NN</td>\n",
       "      <td>toe</td>\n",
       "      <td>O</td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>nn</td>\n",
       "      <td>plastic</td>\n",
       "      <td>21</td>\n",
       "      <td>toe</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207671</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>surgery</td>\n",
       "      <td>NN</td>\n",
       "      <td>surgery</td>\n",
       "      <td>CAUSE_OF_DEATH</td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>dobj</td>\n",
       "      <td>get</td>\n",
       "      <td>14</td>\n",
       "      <td>surgery</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7207672 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Wikipedia_movie_id  Sentence_id     Word  POS    Lemma  \\\n",
       "0                  11784534            1  Bergman  NNP  Bergman   \n",
       "1                  11784534            1  Sanders  NNP  Sanders   \n",
       "2                  11784534            1   couple   NN   couple   \n",
       "3                  11784534            1   couple   NN   couple   \n",
       "4                  11784534            1     gone  VBN       go   \n",
       "...                     ...          ...      ...  ...      ...   \n",
       "7207667             9990262           48      min  NNP      min   \n",
       "7207668             9990262           48      min  NNP      min   \n",
       "7207669             9990262           48     head   NN     head   \n",
       "7207670             9990262           48      toe   NN      toe   \n",
       "7207671             9990262           48  surgery   NN  surgery   \n",
       "\n",
       "                    NER  token_id       Character Dependency_type   Word_1  \\\n",
       "0                PERSON         7  Ingrid Bergman              nn  Bergman   \n",
       "1                PERSON        10  George Sanders              nn  Sanders   \n",
       "2                     O        15                           nsubj   couple   \n",
       "3                     O        15                            amod   couple   \n",
       "4                     O        18                           nsubj     gone   \n",
       "...                 ...       ...             ...             ...      ...   \n",
       "7207667               O        10                           nsubj     asks   \n",
       "7207668               O        10                           nsubj      get   \n",
       "7207669               O        16                              nn  surgery   \n",
       "7207670               O        20                              nn  plastic   \n",
       "7207671  CAUSE_OF_DEATH        22                            dobj      get   \n",
       "\n",
       "         Word_1_idx   Word_2  Word_2_idx  \n",
       "0                 7   Ingrid           6  \n",
       "1                10   George           9  \n",
       "2                15   Joyces           2  \n",
       "3                15  English          14  \n",
       "4                18      who          16  \n",
       "...             ...      ...         ...  \n",
       "7207667          12      min          10  \n",
       "7207668          14      min          10  \n",
       "7207669          22     head          16  \n",
       "7207670          21      toe          20  \n",
       "7207671          14  surgery          22  \n",
       "\n",
       "[7207672 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39ecbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9712.51 MiB, increment: 0.61 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6410.87 MiB, increment: 0.02 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit\n",
    "import gc\n",
    "\n",
    "del coref_fname\n",
    "del coreference\n",
    "del dependencies_fname\n",
    "del filtered_coreference\n",
    "del filtered_entities\n",
    "del filtered_tokens_copy\n",
    "del filtered_dependencies\n",
    "del filtered_tokens\n",
    "del dependencies\n",
    "del dependencies_with_dep\n",
    "del entities\n",
    "del entities_fname\n",
    "del entities_with_character\n",
    "del intersection_set\n",
    "del merge1\n",
    "del merge2 \n",
    "del prefiltered_dependencies\n",
    "del prefiltered_entities\n",
    "del set_dependencies        \n",
    "del set_entities              \n",
    "del set_tokens\n",
    "del tokens\n",
    "del tokens_fname\n",
    "del tokens_with_character\n",
    "gc.collect()\n",
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d90c59",
   "metadata": {},
   "source": [
    "### Adding the POS and lemma of dependencies to the merged dataframe before running extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d44f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_pos_lemma(row):\n",
    "    if row['Character']:\n",
    "        dependency_idx = row['Word_2_idx'] if row['Word'] == row['Word_1'] else row['Word_1_idx']\n",
    "\n",
    "        pos_lemma = final_merged_df.loc[(final_merged_df['Wikipedia_movie_id'] == row['Wikipedia_movie_id']) &\n",
    "                                        (final_merged_df['Sentence_id'] == row['Sentence_id']) &\n",
    "                                        (final_merged_df['token_id'] == dependency_idx), ['POS', 'Lemma']].iloc[0]\n",
    "        return pos_lemma.POS, pos_lemma.Lemma\n",
    "    return '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46be925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 54s\n",
      "Wall time: 2h 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def process_chunk(chunk):\n",
    "    return chunk.apply(get_dependency_pos_lemma, axis=1)\n",
    "\n",
    "num_partitions = joblib.cpu_count()\n",
    "chunk_size = int(np.ceil(final_merged_df.shape[0] / num_partitions))\n",
    "chunks = [final_merged_df.iloc[i:i + chunk_size] for i in range(0, final_merged_df.shape[0], chunk_size)]\n",
    "\n",
    "results = Parallel(n_jobs=num_partitions)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "pos_lemma_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "final_merged_df = pd.concat([final_merged_df.reset_index(drop=True), pos_lemma_df], axis=1)\n",
    "\n",
    "final_merged_df_filtered = final_merged_df[final_merged_df['Character'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df_filtered = final_merged_df_filtered.copy()\n",
    "\n",
    "final_merged_df_filtered.loc[:, 'Dependency_POS'] = final_merged_df_filtered[0].apply(lambda x: x[0])\n",
    "final_merged_df_filtered.loc[:, 'Dependency_Lemma'] = final_merged_df_filtered[0].apply(lambda x: x[1])\n",
    "\n",
    "final_merged_df_filtered = final_merged_df_filtered.drop(columns=[0])\n",
    "\n",
    "final_merged_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "665c278b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2594"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del final_merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d017599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_id</th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>NER</th>\n",
       "      <th>token_id</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dependency_type</th>\n",
       "      <th>Word_1</th>\n",
       "      <th>Word_1_idx</th>\n",
       "      <th>Word_2</th>\n",
       "      <th>Word_2_idx</th>\n",
       "      <th>Dependency_POS</th>\n",
       "      <th>Dependency_Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>nn</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid</td>\n",
       "      <td>6</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Ingrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>10</td>\n",
       "      <td>George Sanders</td>\n",
       "      <td>nn</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>10</td>\n",
       "      <td>George</td>\n",
       "      <td>9</td>\n",
       "      <td>NNP</td>\n",
       "      <td>George</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10131263</td>\n",
       "      <td>1</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>Wile E. Coyote</td>\n",
       "      <td>nn</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>5</td>\n",
       "      <td>Wile</td>\n",
       "      <td>3</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Wile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10131263</td>\n",
       "      <td>1</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>Wile E. Coyote</td>\n",
       "      <td>nn</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>5</td>\n",
       "      <td>E.</td>\n",
       "      <td>4</td>\n",
       "      <td>NNP</td>\n",
       "      <td>E.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10131263</td>\n",
       "      <td>41</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>amod</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>5</td>\n",
       "      <td>charred</td>\n",
       "      <td>4</td>\n",
       "      <td>JJ</td>\n",
       "      <td>charred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207599</th>\n",
       "      <td>9990262</td>\n",
       "      <td>38</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>dobj</td>\n",
       "      <td>encourages</td>\n",
       "      <td>4</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>7</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>encourage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207600</th>\n",
       "      <td>9990262</td>\n",
       "      <td>38</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>do</td>\n",
       "      <td>9</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>7</td>\n",
       "      <td>VB</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207605</th>\n",
       "      <td>9990262</td>\n",
       "      <td>39</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>5</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>sing</td>\n",
       "      <td>8</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>5</td>\n",
       "      <td>VB</td>\n",
       "      <td>sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207611</th>\n",
       "      <td>9990262</td>\n",
       "      <td>40</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>8</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>fake</td>\n",
       "      <td>12</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>8</td>\n",
       "      <td>JJ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207651</th>\n",
       "      <td>9990262</td>\n",
       "      <td>46</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>6</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>dobj</td>\n",
       "      <td>drops</td>\n",
       "      <td>2</td>\n",
       "      <td>Jenny</td>\n",
       "      <td>6</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>drop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948414 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Wikipedia_movie_id  Sentence_id     Word  POS    Lemma     NER  \\\n",
       "0                  11784534            1  Bergman  NNP  Bergman  PERSON   \n",
       "1                  11784534            1  Sanders  NNP  Sanders  PERSON   \n",
       "18                 10131263            1   Coyote  NNP   Coyote  PERSON   \n",
       "19                 10131263            1   Coyote  NNP   Coyote  PERSON   \n",
       "189                10131263           41   Coyote  NNP   Coyote  PERSON   \n",
       "...                     ...          ...      ...  ...      ...     ...   \n",
       "7207599             9990262           38    Jenny  NNP    Jenny  PERSON   \n",
       "7207600             9990262           38    Jenny  NNP    Jenny  PERSON   \n",
       "7207605             9990262           39    Jenny  NNP    Jenny  PERSON   \n",
       "7207611             9990262           40    Jenny  NNP    Jenny  PERSON   \n",
       "7207651             9990262           46    Jenny  NNP    Jenny  PERSON   \n",
       "\n",
       "         token_id       Character Dependency_type      Word_1  Word_1_idx  \\\n",
       "0               7  Ingrid Bergman              nn     Bergman           7   \n",
       "1              10  George Sanders              nn     Sanders          10   \n",
       "18              5  Wile E. Coyote              nn      Coyote           5   \n",
       "19              5  Wile E. Coyote              nn      Coyote           5   \n",
       "189             5          Coyote            amod      Coyote           5   \n",
       "...           ...             ...             ...         ...         ...   \n",
       "7207599         7           Jenny            dobj  encourages           4   \n",
       "7207600         7           Jenny           nsubj          do           9   \n",
       "7207605         5           Jenny           nsubj        sing           8   \n",
       "7207611         8           Jenny           nsubj        fake          12   \n",
       "7207651         6           Jenny            dobj       drops           2   \n",
       "\n",
       "          Word_2  Word_2_idx Dependency_POS Dependency_Lemma  \n",
       "0         Ingrid           6            NNP           Ingrid  \n",
       "1         George           9            NNP           George  \n",
       "18          Wile           3            NNP             Wile  \n",
       "19            E.           4            NNP               E.  \n",
       "189      charred           4             JJ          charred  \n",
       "...          ...         ...            ...              ...  \n",
       "7207599    Jenny           7            VBZ        encourage  \n",
       "7207600    Jenny           7             VB               do  \n",
       "7207605    Jenny           5             VB             sing  \n",
       "7207611    Jenny           8             JJ             fake  \n",
       "7207651    Jenny           6            VBZ             drop  \n",
       "\n",
       "[948414 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8160741",
   "metadata": {},
   "source": [
    "### Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27de19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.DataFrame(columns=['Wikipedia_movie_id', 'Character', 'AV', 'PV', 'Att']) # dataframe where we will store the character, its actions (agent and patient) and attributes\n",
    "verb_pos_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d310235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7 s\n",
      "Wall time: 9.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def process_chunk(chunk):\n",
    "    temp_data = defaultdict(lambda: {'AV': [], 'PV': [], 'Att': []})\n",
    "    for _, row in chunk.iterrows():\n",
    "        result = process_row(row)\n",
    "        key = (result['Wikipedia_movie_id'], result['Character'])\n",
    "        temp_data[key]['AV'].extend(result['AV'])\n",
    "        temp_data[key]['PV'].extend(result['PV'])\n",
    "        temp_data[key]['Att'].extend(result['Att'])\n",
    "    return temp_data\n",
    "\n",
    "# Divide final_merged_df_filtered into chunks\n",
    "num_partitions = joblib.cpu_count()\n",
    "chunk_size = int(np.ceil(final_merged_df_filtered.shape[0] / num_partitions))\n",
    "chunks = [final_merged_df_filtered.iloc[i:i + chunk_size] for i in range(0, final_merged_df_filtered.shape[0], chunk_size)]\n",
    "\n",
    "# Process each chunk in parallel\n",
    "chunk_results = Parallel(n_jobs=num_partitions)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "# Combine the results from all chunks\n",
    "combined_results = defaultdict(lambda: {'AV': [], 'PV': [], 'Att': []})\n",
    "for chunk_result in chunk_results:\n",
    "    for key, values in chunk_result.items():\n",
    "        combined_results[key]['AV'].extend(values['AV'])\n",
    "        combined_results[key]['PV'].extend(values['PV'])\n",
    "        combined_results[key]['Att'].extend(values['Att'])\n",
    "\n",
    "# Convert combined_results to DataFrame\n",
    "character_data = []\n",
    "for (movie_id, character), values in combined_results.items():\n",
    "    character_data.append({\n",
    "        'Wikipedia_movie_id': movie_id,\n",
    "        'Character': character,\n",
    "        'AV': values['AV'],\n",
    "        'PV': values['PV'],\n",
    "        'Att': values['Att']\n",
    "    })\n",
    "\n",
    "characters = pd.DataFrame(character_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3e0dbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_id</th>\n",
       "      <th>Character</th>\n",
       "      <th>AV</th>\n",
       "      <th>PV</th>\n",
       "      <th>Att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11784534</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Ingrid, Bergman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11784534</td>\n",
       "      <td>George Sanders</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[George, Sanders]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10131263</td>\n",
       "      <td>Wile E. Coyote</td>\n",
       "      <td>[cook]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wile, E., Coyote, Coyote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10131263</td>\n",
       "      <td>Coyote</td>\n",
       "      <td>[walk]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[charred, gullible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1067527</td>\n",
       "      <td>Lau</td>\n",
       "      <td>[handle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[one, inspector]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229415</th>\n",
       "      <td>999394</td>\n",
       "      <td>Bootstrap</td>\n",
       "      <td>[tell]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229416</th>\n",
       "      <td>999394</td>\n",
       "      <td>Dalma</td>\n",
       "      <td>[tell, say]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229417</th>\n",
       "      <td>999394</td>\n",
       "      <td>Gibbs</td>\n",
       "      <td>[encounter, hire]</td>\n",
       "      <td>[tell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229418</th>\n",
       "      <td>999394</td>\n",
       "      <td>Barbossa</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[captain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229419</th>\n",
       "      <td>9990262</td>\n",
       "      <td>Amy</td>\n",
       "      <td>[humiliate, try, find, realize, bring]</td>\n",
       "      <td>[tell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Wikipedia_movie_id       Character  \\\n",
       "0                 11784534  Ingrid Bergman   \n",
       "1                 11784534  George Sanders   \n",
       "2                 10131263  Wile E. Coyote   \n",
       "3                 10131263          Coyote   \n",
       "4                  1067527             Lau   \n",
       "...                    ...             ...   \n",
       "229415              999394       Bootstrap   \n",
       "229416              999394           Dalma   \n",
       "229417              999394           Gibbs   \n",
       "229418              999394        Barbossa   \n",
       "229419             9990262             Amy   \n",
       "\n",
       "                                            AV      PV  \\\n",
       "0                                           []      []   \n",
       "1                                           []      []   \n",
       "2                                       [cook]      []   \n",
       "3                                       [walk]      []   \n",
       "4                                     [handle]      []   \n",
       "...                                        ...     ...   \n",
       "229415                                  [tell]      []   \n",
       "229416                             [tell, say]      []   \n",
       "229417                       [encounter, hire]  [tell]   \n",
       "229418                                      []      []   \n",
       "229419  [humiliate, try, find, realize, bring]  [tell]   \n",
       "\n",
       "                               Att  \n",
       "0                [Ingrid, Bergman]  \n",
       "1                [George, Sanders]  \n",
       "2       [Wile, E., Coyote, Coyote]  \n",
       "3              [charred, gullible]  \n",
       "4                 [one, inspector]  \n",
       "...                            ...  \n",
       "229415                          []  \n",
       "229416                          []  \n",
       "229417                          []  \n",
       "229418                   [captain]  \n",
       "229419                          []  \n",
       "\n",
       "[229420 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bc01683",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.to_parquet(\"characters_new_annotations.parquet\", compression=\"Brotli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897ae88",
   "metadata": {},
   "source": [
    "### Generate bag of words\n",
    "A bag is a tupple of $(r,w)$, where $r$ is of {agent verb, patient verb, attribute} and $w$ is the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d63f1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3469284f314a4dd79a17f80fdab636ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.77 s\n",
      "Wall time: 7.37 s\n"
     ]
    }
   ],
   "source": [
    "def generate_bags_of_words(characters_df: pd.DataFrame):\n",
    "    bags_of_words = []\n",
    "\n",
    "    for _, row in tqdm(characters_df.iterrows()):\n",
    "        movie_id = row['Wikipedia_movie_id']\n",
    "        character_name = row['Character']\n",
    "\n",
    "        av = row['AV'] if isinstance(row['AV'], list) else []\n",
    "        pv = row['PV'] if isinstance(row['PV'], list) else []\n",
    "        att = row['Att'] if isinstance(row['Att'], list) else []\n",
    "\n",
    "        for verb in av:\n",
    "            bags_of_words.append((movie_id, character_name, 'agent_verb', verb))\n",
    "\n",
    "        for verb in pv:\n",
    "            bags_of_words.append((movie_id, character_name, 'patient_verb', verb))\n",
    "\n",
    "        for attribute in att:\n",
    "            bags_of_words.append((movie_id, character_name, 'attribute', attribute))\n",
    "\n",
    "    return bags_of_words\n",
    "\n",
    "bags_of_words = generate_bags_of_words(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3f20245",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_df = pd.DataFrame(bags_of_words, columns=['movie_id', 'character_name', 'type', 'word'])\n",
    "bags_df.to_parquet('bags_new_annotations.parquet', compression='Brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc210d5",
   "metadata": {},
   "source": [
    "### From tupples to topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_anno_results_path=os.path.join(PATH_IN,'new_annotations_results')\n",
    "bags_df_path = os.path.join(old_anno_results_path, 'bags_new_annotations.parquet')\n",
    "bags_df = pd.read_parquet(bags_df_path)\n",
    "bags_of_words=bags_df.values\n",
    "\n",
    "bags_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96273935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# global char version\n",
    "character_docs = defaultdict(list)\n",
    "for _, character, _, word in bags_of_words:\n",
    "    character_docs[character].append(word)\n",
    "    \n",
    "for character in character_docs:\n",
    "    character_docs[character] = \" \".join(character_docs[character])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa624433",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(character_docs.values()) # can probably remove it, and directly use character_docs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15287f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, verbose=2, max_iter=10, random_state = 0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lda, 'lda_model_new_annotations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119957df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#Load LDA trained model\n",
    "lda = joblib.load(os.path.join(PATH_IN,'old_annotations_results/lda_model_new_annotations.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        \n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9199ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_topic = lda.transform(X)\n",
    "character_names = list(character_docs.keys())\n",
    "\n",
    "for i, topic_dist in enumerate(character_topic):\n",
    "    topic_most_pr = topic_dist.argmax()\n",
    "    print(f\"{character_names[i]}: Topic {topic_most_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plot for a char => cluster result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
