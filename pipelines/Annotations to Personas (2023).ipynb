{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f74e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from helpers.readers import read_dataframe\n",
    "\n",
    "DATA_PATH = '../generated/annotations_2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed23b7",
   "metadata": {},
   "source": [
    "### How to get the data\n",
    "1. Download from [here](https://drive.google.com/drive/folders/1pSyWy1uQqbTe4xCt9TtvIQM0YTyVZ8Tr?usp=drive_link) `tokens.parquet`, `dependencies.parquet` and `entities.parquet` (generated from [`Text to Dataframes (2023).ipnyb`](https://github.com/epfl-ada/ada-2023-project-crunchychicken/blob/main/pipelines/Text%20to%20Dataframes%20(2023).ipynb))\n",
    "2. Place the files inside `annotations_2023/`:\n",
    "```\n",
    "project_root/\n",
    "│\n",
    "├── P2.ipynb\n",
    "│\n",
    "├── generated/\n",
    "│   ├── annotations_2023/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0766a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = read_dataframe('cmu/tokens_2023')\n",
    "dependencies = read_dataframe('cmu/dependencies_2023')\n",
    "entities = read_dataframe('cmu/entities_2023')\n",
    "# coreference is complex to integrate, for now we will use these three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e60bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['token_id'] = tokens.groupby(['Wikipedia_movie_id', 'Sentence_id']).cumcount() + 1 # adding token id, not present by default in 4.5.5\n",
    "tokens = tokens.drop(['Sentiment', 'COB', 'COE'], axis=1) # can use Sentiment later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336cb08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15046378, 15376339, 2220379)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15M            15M                 2.2M\n",
    "len(tokens), len(dependencies), len(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c5a42",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Running on such big dataframes will take too much time. To solve this, we will first filter on movie plots that contain a character (from the entities dataframe check for `Entity_type == PERSON` and a non NaN `Optional_probability`) and on also filter on the dependencies (from the dependencies dataframe check that the `Dependency_type` satisfies an agent verb, patient verb or attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c69e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.4 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent_verbs = [\"agent\", \"nsubj\"]\n",
    "patient_verbs = [\"dobj\", \"nsubjpass\", \"iobj\"] # no prep_ using coreNLP4.5.5\n",
    "attributes_av = [\"nsubj\", \"appos\"]\n",
    "attributes_pv = [\"nsubj\", \"appos\", \"amod\", \"nn\"]\n",
    "\n",
    "def is_matching_dependency(dep_type):\n",
    "    if dep_type in agent_verbs or dep_type in patient_verbs or dep_type in attributes_av or dep_type in attributes_pv:\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "tokens_with_character = tokens[tokens[\"NER\"] == \"PERSON\"][\"Wikipedia_movie_id\"].tolist()\n",
    "entities_with_character = entities[(entities['Entity_type'] == 'PERSON') & (entities['Optional_probability'].notna())][\"Wikipedia_movie_id\"].tolist()\n",
    "prefiltered_entities = entities[(entities['Entity_type'] == 'PERSON') & (entities['Optional_probability'].notna())].copy()\n",
    "dependencies_with_dep = dependencies[dependencies['Dependency_type'].apply(is_matching_dependency)][\"Wikipedia_movie_id\"].tolist()\n",
    "prefiltered_dependencies = dependencies[dependencies['Dependency_type'].apply(is_matching_dependency)].copy()\n",
    "\n",
    "set_tokens = set(tokens_with_character)\n",
    "set_entities = set(entities_with_character)\n",
    "set_dependencies = set(dependencies_with_dep)\n",
    "\n",
    "intersection_set = set_tokens & set_entities & set_dependencies\n",
    "\n",
    "filtered_tokens = tokens[tokens[\"Wikipedia_movie_id\"].isin(intersection_set)]\n",
    "filtered_entities = prefiltered_entities[prefiltered_entities[\"Wikipedia_movie_id\"].isin(intersection_set)]\n",
    "filtered_dependencies = prefiltered_dependencies[prefiltered_dependencies[\"Wikipedia_movie_id\"].isin(intersection_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e81993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14714763, 3603836, 848088)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tokens), len(filtered_dependencies), len(filtered_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc7077d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 22s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_to_character = {}\n",
    "for _, row in filtered_entities.iterrows():\n",
    "    character_name = row['Word']\n",
    "    movie_id = row['Wikipedia_movie_id']\n",
    "    sentence_id = row['Sentence_id']\n",
    "    for word in character_name.split():\n",
    "        key = (movie_id, sentence_id, word)\n",
    "        word_to_character[key] = character_name\n",
    "\n",
    "filtered_tokens_copy = filtered_tokens.copy()\n",
    "\n",
    "def map_word_to_character(row):\n",
    "    key = (row['Wikipedia_movie_id'], row['Sentence_id'], row['Word'])\n",
    "    return word_to_character.get(key, '')\n",
    "\n",
    "filtered_tokens_copy['Character'] = filtered_tokens_copy.apply(map_word_to_character, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e899fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.5 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge1 = pd.merge(filtered_tokens_copy, filtered_dependencies, left_on=['Wikipedia_movie_id', 'Sentence_id', 'token_id'], right_on=['Wikipedia_movie_id', 'Sentence_id', 'Word_1_idx'])\n",
    "\n",
    "merge2 = pd.merge(filtered_tokens_copy, filtered_dependencies, left_on=['Wikipedia_movie_id', 'Sentence_id', 'token_id'], right_on=['Wikipedia_movie_id', 'Sentence_id', 'Word_2_idx'])\n",
    "\n",
    "final_merged_df = pd.concat([merge1, merge2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0295d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_id</th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>NER</th>\n",
       "      <th>token_id</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dependency_type</th>\n",
       "      <th>Word_1</th>\n",
       "      <th>Word_1_idx</th>\n",
       "      <th>Word_2</th>\n",
       "      <th>Word_2_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>nn</td>\n",
       "      <td>Bergman</td>\n",
       "      <td>7</td>\n",
       "      <td>Ingrid</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>10</td>\n",
       "      <td>George Sanders</td>\n",
       "      <td>nn</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>10</td>\n",
       "      <td>George</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>couple</td>\n",
       "      <td>NN</td>\n",
       "      <td>couple</td>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>couple</td>\n",
       "      <td>15</td>\n",
       "      <td>Joyces</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>couple</td>\n",
       "      <td>NN</td>\n",
       "      <td>couple</td>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>amod</td>\n",
       "      <td>couple</td>\n",
       "      <td>15</td>\n",
       "      <td>English</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11784534</td>\n",
       "      <td>1</td>\n",
       "      <td>gone</td>\n",
       "      <td>VBN</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>gone</td>\n",
       "      <td>18</td>\n",
       "      <td>who</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207667</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>min</td>\n",
       "      <td>NNP</td>\n",
       "      <td>min</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>asks</td>\n",
       "      <td>12</td>\n",
       "      <td>min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207668</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>min</td>\n",
       "      <td>NNP</td>\n",
       "      <td>min</td>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>nsubj</td>\n",
       "      <td>get</td>\n",
       "      <td>14</td>\n",
       "      <td>min</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207669</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>head</td>\n",
       "      <td>NN</td>\n",
       "      <td>head</td>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>nn</td>\n",
       "      <td>surgery</td>\n",
       "      <td>22</td>\n",
       "      <td>head</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207670</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>toe</td>\n",
       "      <td>NN</td>\n",
       "      <td>toe</td>\n",
       "      <td>O</td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>nn</td>\n",
       "      <td>plastic</td>\n",
       "      <td>21</td>\n",
       "      <td>toe</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207671</th>\n",
       "      <td>9990262</td>\n",
       "      <td>48</td>\n",
       "      <td>surgery</td>\n",
       "      <td>NN</td>\n",
       "      <td>surgery</td>\n",
       "      <td>CAUSE_OF_DEATH</td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>dobj</td>\n",
       "      <td>get</td>\n",
       "      <td>14</td>\n",
       "      <td>surgery</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7207672 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Wikipedia_movie_id  Sentence_id     Word  POS    Lemma  \\\n",
       "0                  11784534            1  Bergman  NNP  Bergman   \n",
       "1                  11784534            1  Sanders  NNP  Sanders   \n",
       "2                  11784534            1   couple   NN   couple   \n",
       "3                  11784534            1   couple   NN   couple   \n",
       "4                  11784534            1     gone  VBN       go   \n",
       "...                     ...          ...      ...  ...      ...   \n",
       "7207667             9990262           48      min  NNP      min   \n",
       "7207668             9990262           48      min  NNP      min   \n",
       "7207669             9990262           48     head   NN     head   \n",
       "7207670             9990262           48      toe   NN      toe   \n",
       "7207671             9990262           48  surgery   NN  surgery   \n",
       "\n",
       "                    NER  token_id       Character Dependency_type   Word_1  \\\n",
       "0                PERSON         7  Ingrid Bergman              nn  Bergman   \n",
       "1                PERSON        10  George Sanders              nn  Sanders   \n",
       "2                     O        15                           nsubj   couple   \n",
       "3                     O        15                            amod   couple   \n",
       "4                     O        18                           nsubj     gone   \n",
       "...                 ...       ...             ...             ...      ...   \n",
       "7207667               O        10                           nsubj     asks   \n",
       "7207668               O        10                           nsubj      get   \n",
       "7207669               O        16                              nn  surgery   \n",
       "7207670               O        20                              nn  plastic   \n",
       "7207671  CAUSE_OF_DEATH        22                            dobj      get   \n",
       "\n",
       "         Word_1_idx   Word_2  Word_2_idx  \n",
       "0                 7   Ingrid           6  \n",
       "1                10   George           9  \n",
       "2                15   Joyces           2  \n",
       "3                15  English          14  \n",
       "4                18      who          16  \n",
       "...             ...      ...         ...  \n",
       "7207667          12      min          10  \n",
       "7207668          14      min          10  \n",
       "7207669          22     head          16  \n",
       "7207670          21      toe          20  \n",
       "7207671          14  surgery          22  \n",
       "\n",
       "[7207672 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39ecbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 10949.94 MiB, increment: 0.04 MiB\n",
      "peak memory: 6863.02 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# !pip install memory_profiler\n",
    "# install and load memory_profiler to use %memit, use %whos to see what's in memory\n",
    "%load_ext memory_profiler\n",
    "%memit\n",
    "import gc\n",
    "\n",
    "del filtered_entities\n",
    "del filtered_tokens_copy\n",
    "del filtered_dependencies\n",
    "del filtered_tokens\n",
    "del dependencies\n",
    "del dependencies_with_dep\n",
    "del entities\n",
    "del entities_with_character\n",
    "del intersection_set\n",
    "del merge1\n",
    "del merge2 \n",
    "del prefiltered_dependencies\n",
    "del prefiltered_entities\n",
    "del set_dependencies        \n",
    "del set_entities              \n",
    "del set_tokens\n",
    "del tokens\n",
    "del tokens_with_character\n",
    "\n",
    "gc.collect()\n",
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d90c59",
   "metadata": {},
   "source": [
    "### Adding the POS and lemma of dependencies to the merged dataframe before running extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_pos_lemma(row):\n",
    "    if row['Character']:\n",
    "        dependency_idx = row['Word_2_idx'] if row['Word'] == row['Word_1'] else row['Word_1_idx']\n",
    "\n",
    "        pos_lemma = final_merged_df.loc[(final_merged_df['Wikipedia_movie_id'] == row['Wikipedia_movie_id']) &\n",
    "                                        (final_merged_df['Sentence_id'] == row['Sentence_id']) &\n",
    "                                        (final_merged_df['token_id'] == dependency_idx), ['POS', 'Lemma']].iloc[0]\n",
    "        return pos_lemma.POS, pos_lemma.Lemma\n",
    "    return '', ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be5aac",
   "metadata": {},
   "source": [
    "Will take around 2 hours 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def process_chunk(chunk):\n",
    "    return chunk.apply(get_dependency_pos_lemma, axis=1)\n",
    "\n",
    "num_partitions = joblib.cpu_count()\n",
    "chunk_size = int(np.ceil(final_merged_df.shape[0] / num_partitions))\n",
    "chunks = [final_merged_df.iloc[i:i + chunk_size] for i in range(0, final_merged_df.shape[0], chunk_size)]\n",
    "\n",
    "results = Parallel(n_jobs=num_partitions)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "pos_lemma_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "final_merged_df = pd.concat([final_merged_df.reset_index(drop=True), pos_lemma_df], axis=1)\n",
    "\n",
    "final_merged_df_filtered = final_merged_df[final_merged_df['Character'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df_filtered = final_merged_df_filtered.copy()\n",
    "\n",
    "final_merged_df_filtered.loc[:, 'Dependency_POS'] = final_merged_df_filtered[0].apply(lambda x: x[0])\n",
    "final_merged_df_filtered.loc[:, 'Dependency_Lemma'] = final_merged_df_filtered[0].apply(lambda x: x[1])\n",
    "\n",
    "final_merged_df_filtered = final_merged_df_filtered.drop(columns=[0])\n",
    "\n",
    "final_merged_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del results\n",
    "del pos_lemma_df\n",
    "del final_merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8160741",
   "metadata": {},
   "source": [
    "### Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.DataFrame(columns=['Wikipedia_movie_id', 'Character', 'AV', 'PV', 'Att']) # dataframe where we will store the character, its actions (agent and patient) and attributes\n",
    "verb_pos_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"] # to indentify verbs from attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d310235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def process_chunk(chunk):\n",
    "    temp_data = defaultdict(lambda: {'AV': [], 'PV': [], 'Att': []})\n",
    "    for _, row in chunk.iterrows():\n",
    "        result = process_row(row)\n",
    "        key = (result['Wikipedia_movie_id'], result['Character'])\n",
    "        temp_data[key]['AV'].extend(result['AV'])\n",
    "        temp_data[key]['PV'].extend(result['PV'])\n",
    "        temp_data[key]['Att'].extend(result['Att'])\n",
    "    return temp_data\n",
    "\n",
    "num_partitions = joblib.cpu_count()\n",
    "chunk_size = int(np.ceil(final_merged_df_filtered.shape[0] / num_partitions))\n",
    "chunks = [final_merged_df_filtered.iloc[i:i + chunk_size] for i in range(0, final_merged_df_filtered.shape[0], chunk_size)]\n",
    "\n",
    "chunk_results = Parallel(n_jobs=num_partitions)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "combined_results = defaultdict(lambda: {'AV': [], 'PV': [], 'Att': []})\n",
    "for chunk_result in chunk_results:\n",
    "    for key, values in chunk_result.items():\n",
    "        combined_results[key]['AV'].extend(values['AV'])\n",
    "        combined_results[key]['PV'].extend(values['PV'])\n",
    "        combined_results[key]['Att'].extend(values['Att'])\n",
    "\n",
    "character_data = []\n",
    "for (movie_id, character), values in combined_results.items():\n",
    "    character_data.append({\n",
    "        'Wikipedia_movie_id': movie_id,\n",
    "        'Character': character,\n",
    "        'AV': values['AV'],\n",
    "        'PV': values['PV'],\n",
    "        'Att': values['Att']\n",
    "    })\n",
    "\n",
    "characters = pd.DataFrame(character_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89468035",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.to_parquet(os.path.join(DATA_PATH, \"characters.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897ae88",
   "metadata": {},
   "source": [
    "### Generate bag of words\n",
    "A bag is a tupple of $(r,w)$, where $r$ is of {agent verb, patient verb, attribute} and $w$ is the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bags_of_words(characters_df: pd.DataFrame):\n",
    "    bags_of_words = []\n",
    "\n",
    "    for _, row in tqdm(characters_df.iterrows()):\n",
    "        movie_id = row['Wikipedia_movie_id']\n",
    "        character_name = row['Character']\n",
    "\n",
    "        av = row['AV'] if isinstance(row['AV'], list) else []\n",
    "        pv = row['PV'] if isinstance(row['PV'], list) else []\n",
    "        att = row['Att'] if isinstance(row['Att'], list) else []\n",
    "\n",
    "        for verb in av:\n",
    "            bags_of_words.append((movie_id, character_name, 'agent_verb', verb))\n",
    "\n",
    "        for verb in pv:\n",
    "            bags_of_words.append((movie_id, character_name, 'patient_verb', verb))\n",
    "\n",
    "        for attribute in att:\n",
    "            bags_of_words.append((movie_id, character_name, 'attribute', attribute))\n",
    "\n",
    "    return bags_of_words\n",
    "\n",
    "bags_of_words = generate_bags_of_words(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f20245",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_df = pd.DataFrame(bags_of_words, columns=['movie_id', 'character_name', 'type', 'word'])\n",
    "bags_df.to_parquet(os.path.join(DATA_PATH, \"bags.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc210d5",
   "metadata": {},
   "source": [
    "### From tupples to topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0509f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>character_name</th>\n",
       "      <th>type</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11784534</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Ingrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11784534</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>attribute</td>\n",
       "      <td>Bergman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11784534</td>\n",
       "      <td>George Sanders</td>\n",
       "      <td>attribute</td>\n",
       "      <td>George</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id  character_name       type     word\n",
       "0  11784534  Ingrid Bergman  attribute   Ingrid\n",
       "1  11784534  Ingrid Bergman  attribute  Bergman\n",
       "2  11784534  George Sanders  attribute   George"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags_df = read_dataframe('cmu/bags_2023')\n",
    "bags_of_words=bags_df.values\n",
    "\n",
    "bags_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96273935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 6 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "# currently global character version (doesn't make the distinction of the same character in different movies)\n",
    "character_docs = defaultdict(list)\n",
    "for _, character, _, word in bags_of_words:\n",
    "    character_docs[character].append(word)\n",
    "    \n",
    "for character in character_docs:\n",
    "    character_docs[character] = \" \".join(character_docs[character])\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(character_docs.values())\n",
    "\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, verbose=2, max_iter=10, random_state = 0)\n",
    "lda.fit(X)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(character_docs.values())\n",
    "\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, verbose=2, max_iter=10, random_state = 0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af64b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lda, os.path.join(DATA_PATH, \"lda_model.gz\"), compress=('gzip', 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = joblib.load(os.path.join(DATA_PATH, \"lda_model.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic {topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "n_topic_words = 10\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), n_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9199ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_topic = lda.transform(X)\n",
    "character_names = list(character_docs.keys())\n",
    "\n",
    "character_classification=[]\n",
    "\n",
    "for i, topic_dist in enumerate(character_topic):\n",
    "    topic_most_prob = topic_dist.argmax()\n",
    "    character_classification.append((character_names[i], topic_most_prob, topic_dist))\n",
    "\n",
    "character_classification_df=pd.DataFrame(character_classification,columns=['character_name', 'topic', 'topic_dist'])\n",
    "character_classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_classification_df.to_parquet(os.path.join(DATA_PATH, \"character_classification.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "character='Batman'\n",
    "\n",
    "character_topics = character_classification_df[character_classification_df['character_name'] == character]['topic_dist'].iloc[0]\n",
    "\n",
    "topics = range(len(character_topics))\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topics, character_topics)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.title(f'Topic Distribution for {character}')\n",
    "plt.yscale('log')  \n",
    "plt.xticks(topics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = character_classification_df['topic'].value_counts()\n",
    "\n",
    "topic_counts = topic_counts.sort_index()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topic_counts.index, topic_counts.values)\n",
    "\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Characters')\n",
    "plt.title('Distribution of Characters Across Topics')\n",
    "plt.xticks(topic_counts.index)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
