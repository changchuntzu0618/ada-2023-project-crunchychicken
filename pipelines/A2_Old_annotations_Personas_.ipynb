{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a3b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('..')\n",
    "from helpers.readers import read_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1829337",
   "metadata": {},
   "source": [
    "## add data download link and data placement instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85045515",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IN = '../generated/annotations_2013/'\n",
    "\n",
    "tokens = read_dataframe('cmu/tokens_2013')\n",
    "dependencies = read_dataframe('cmu/dependencies_2013')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c2882",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Find movies that contain at least one character (from tokens check for atleast one `NER==\"PERSON\"`) and check that dependencies contain only the dependencies we are looking for as in *Learning Latent Personas of Film Characters* from David Bamman Brendan Oâ€™Connor Noah A. Smith.\n",
    "\n",
    "![dep_needed](https://i.postimg.cc/sfbxBMV6/image-2023-11-15-113351627.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34686580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[tokens[\"NER\"] == \"PERSON\"].head(3) # using old annotations, NER can miss some characters, for example label them as ORGANIZATION\n",
    "movies_with_atleast_one_person = tokens[tokens[\"NER\"] == \"PERSON\"][\"movie_id\"].unique().tolist()\n",
    "len(movies_with_atleast_one_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1ad268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.2 s\n",
      "Wall time: 17.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42304"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agent_verbs = [\"agent\", \"nsubj\"]\n",
    "patient_verbs = [\"dobj\", \"nsubjpass\", \"iobj\"] # + prep_* (handled seperatly)\n",
    "attributes_av = [\"nsubj\", \"appos\"]\n",
    "attributes_pv = [\"nsubj\", \"appos\", \"amod\", \"nn\"]\n",
    "\n",
    "dependencies_needed = dependencies[(dependencies[\"dependency_class\"] == \"collapsed-ccprocessed\") & \n",
    "                                   ((dependencies[\"dependency_type\"].isin(agent_verbs + patient_verbs + attributes_av + attributes_pv)) |  \n",
    "                                    (dependencies[\"dependency_type\"].str.startswith(\"prep_\")))].copy()\n",
    "\n",
    "dependencies_needed.head(3)\n",
    "\n",
    "movies_with_atleast_one_dep = dependencies_needed[\"movie_id\"].unique().tolist()\n",
    "\n",
    "len(movies_with_atleast_one_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6697fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 13.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "set_movies_with_person = set(movies_with_atleast_one_person)\n",
    "set_movies_with_dep = set(movies_with_atleast_one_dep)\n",
    "\n",
    "common_movies = set_movies_with_person.intersection(set_movies_with_dep)\n",
    "\n",
    "common_movies_list = list(common_movies)\n",
    "\n",
    "len(common_movies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = tokens[tokens[\"movie_id\"].isin(common_movies_list)].copy()\n",
    "filtered_dependencies = dependencies_needed[dependencies_needed[\"movie_id\"].isin(common_movies_list)].copy()\n",
    "\n",
    "filtered_tokens = filtered_tokens.drop([\"COB\", \"COE\"], axis=1)\n",
    "filtered_dependencies = filtered_dependencies.drop([\"dependency_class\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecd3731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "      <th>dependency_type</th>\n",
       "      <th>governor_id</th>\n",
       "      <th>governor_word</th>\n",
       "      <th>dependent_id</th>\n",
       "      <th>dependent_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000053</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>te</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>1</td>\n",
       "      <td>Fur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000053</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>te</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>2</td>\n",
       "      <td>trapper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000053</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>te</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>3</td>\n",
       "      <td>Jean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000053</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>te</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>4</td>\n",
       "      <td>La</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000053</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>te</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>6</td>\n",
       "      <td>te</td>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077137</th>\n",
       "      <td>9999280</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>reasons</td>\n",
       "      <td>reason</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>dobj</td>\n",
       "      <td>3</td>\n",
       "      <td>discovers</td>\n",
       "      <td>5</td>\n",
       "      <td>reasons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077138</th>\n",
       "      <td>9999280</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>separation</td>\n",
       "      <td>separation</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>prep_for</td>\n",
       "      <td>5</td>\n",
       "      <td>reasons</td>\n",
       "      <td>10</td>\n",
       "      <td>separation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077139</th>\n",
       "      <td>9999280</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>Marcelo</td>\n",
       "      <td>Marcelo</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>nn</td>\n",
       "      <td>15</td>\n",
       "      <td>reality</td>\n",
       "      <td>13</td>\n",
       "      <td>Marcelo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077140</th>\n",
       "      <td>9999280</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>face</td>\n",
       "      <td>face</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nn</td>\n",
       "      <td>15</td>\n",
       "      <td>reality</td>\n",
       "      <td>14</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10077141</th>\n",
       "      <td>9999280</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>reality</td>\n",
       "      <td>reality</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>17</td>\n",
       "      <td>clearly</td>\n",
       "      <td>15</td>\n",
       "      <td>reality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10077142 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie_id  sentence_id  token_id        word       lemma  POS  \\\n",
       "0         10000053            1         6          te          te   NN   \n",
       "1         10000053            1         6          te          te   NN   \n",
       "2         10000053            1         6          te          te   NN   \n",
       "3         10000053            1         6          te          te   NN   \n",
       "4         10000053            1         6          te          te   NN   \n",
       "...            ...          ...       ...         ...         ...  ...   \n",
       "10077137   9999280            6         5     reasons      reason  NNS   \n",
       "10077138   9999280            6        10  separation  separation   NN   \n",
       "10077139   9999280            6        13     Marcelo     Marcelo  NNP   \n",
       "10077140   9999280            6        14        face        face   NN   \n",
       "10077141   9999280            6        15     reality     reality   NN   \n",
       "\n",
       "             NER dependency_type  governor_id governor_word  dependent_id  \\\n",
       "0              O              nn            6            te             1   \n",
       "1              O              nn            6            te             2   \n",
       "2              O              nn            6            te             3   \n",
       "3              O              nn            6            te             4   \n",
       "4              O              nn            6            te             5   \n",
       "...          ...             ...          ...           ...           ...   \n",
       "10077137       O            dobj            3     discovers             5   \n",
       "10077138       O        prep_for            5       reasons            10   \n",
       "10077139  PERSON              nn           15       reality            13   \n",
       "10077140       O              nn           15       reality            14   \n",
       "10077141       O           nsubj           17       clearly            15   \n",
       "\n",
       "         dependent_word  \n",
       "0                   Fur  \n",
       "1               trapper  \n",
       "2                  Jean  \n",
       "3                    La  \n",
       "4                     B  \n",
       "...                 ...  \n",
       "10077137        reasons  \n",
       "10077138     separation  \n",
       "10077139        Marcelo  \n",
       "10077140           face  \n",
       "10077141        reality  \n",
       "\n",
       "[10077142 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "merge1 = pd.merge(filtered_tokens, filtered_dependencies,\n",
    "                 left_on=['movie_id', 'sentence_id', 'token_id'],\n",
    "                 right_on=['movie_id', 'sentence_id', 'governor_id'],\n",
    "                 how='inner')\n",
    "\n",
    "merge2 = pd.merge(filtered_tokens, filtered_dependencies,\n",
    "                 left_on=['movie_id', 'sentence_id', 'token_id'],\n",
    "                 right_on=['movie_id', 'sentence_id', 'dependent_id'],\n",
    "                 how='inner')\n",
    "\n",
    "merged_data = pd.concat([merge1, merge2]).drop_duplicates().reset_index(drop=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1097a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "      <th>dependency_type</th>\n",
       "      <th>governor_id</th>\n",
       "      <th>governor_word</th>\n",
       "      <th>dependent_id</th>\n",
       "      <th>dependent_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [movie_id, sentence_id, token_id, word, lemma, POS, NER, dependency_type, governor_id, governor_word, dependent_id, dependent_word]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[(merged_data[\"token_id\"] != merged_data[\"governor_id\"]) & (merged_data[\"token_id\"] != merged_data[\"dependent_id\"])] # should be empty âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d89fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 16578.92 MiB, increment: 0.06 MiB\n",
      "peak memory: 10213.35 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# !pip install memory_profiler\n",
    "\n",
    "# install and load memory_profiler to use %memit, use %whos to see what's in memory\n",
    "%load_ext memory_profiler \n",
    "%memit\n",
    "import gc\n",
    "\n",
    "del common_movies\n",
    "del set_movies_with_dep\n",
    "del set_movies_with_person\n",
    "del common_movies_list\n",
    "del movies_with_atleast_one_dep\n",
    "del movies_with_atleast_one_person\n",
    "\n",
    "del merge1\n",
    "del merge2\n",
    "\n",
    "del filtered_tokens\n",
    "del filtered_dependencies\n",
    "\n",
    "del dependencies_needed\n",
    "del dependencies\n",
    "del tokens\n",
    "\n",
    "gc.collect()\n",
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf179a",
   "metadata": {},
   "source": [
    "### For each plot:\n",
    "1. Extract the characters (NER=PERSON)\n",
    "2. For each character extract (dependencies + coreference)\n",
    "    - agent verbs\n",
    "    - patient verbs\n",
    "    - attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71773e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.DataFrame(columns=['movie_id', 'character', 'AV', 'PV', 'Att']) # dataframe where we will store the character, its actions (agent and patient) and attributes\n",
    "verb_pos_tags = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"] # to indentify verbs from attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7992b",
   "metadata": {},
   "source": [
    "### Extractor using parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# there are more precise ways to do it, but as we are using not very accurate annotations data, we will spend more time enhancing the new annotations.\n",
    "\n",
    "def process_group(chunk):\n",
    "    temp_data = []\n",
    "    for (movie_id, character), group in chunk.groupby(['movie_id', 'word']):\n",
    "        AV, PV, Att = [], [], [] \n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            if row['governor_word'] == character:\n",
    "                dependency_word = row['dependent_word']\n",
    "                dependency_idx = row['dependent_id']\n",
    "            elif row['dependent_word'] == character:\n",
    "                dependency_word = row['governor_word']\n",
    "                dependency_idx = row['governor_id']\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "\n",
    "            pos_row = merged_data[(merged_data['movie_id'] == movie_id) & \n",
    "                                  (merged_data['sentence_id'] == row['sentence_id']) & \n",
    "                                  (merged_data['token_id'] == dependency_idx)]\n",
    "\n",
    "            if not pos_row.empty and pos_row.iloc[0]['POS'] in verb_pos_tags:\n",
    "                if row['dependency_type'] in agent_verbs or row['dependency_type'].startswith(\"prep_\"):\n",
    "                    AV.append(pos_row.iloc[0]['lemma'])\n",
    "                elif row['dependency_type'] in patient_verbs:\n",
    "                    PV.append(pos_row.iloc[0]['lemma'])\n",
    "            else:\n",
    "                Att.append(pos_row.iloc[0]['lemma'])\n",
    "\n",
    "        temp_data.append({\n",
    "            'movie_id': movie_id,\n",
    "            'character': character,\n",
    "            'AV': AV,\n",
    "            'PV': PV,\n",
    "            'Att': Att\n",
    "        })\n",
    "    return temp_data\n",
    "\n",
    "character_data = merged_data[merged_data[\"NER\"] == \"PERSON\"]\n",
    "\n",
    "num_partitions = joblib.cpu_count() \n",
    "chunk_size = int(character_data.shape[0] / num_partitions)\n",
    "chunks = [character_data.iloc[i:i + chunk_size] for i in range(0, character_data.shape[0], chunk_size)]\n",
    "\n",
    "results = Parallel(n_jobs=num_partitions)(delayed(process_group)(chunk) for chunk in tqdm(chunks))\n",
    "\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "characters = pd.concat([pd.DataFrame(flattened_results)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab952fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.to_parquet(os.path.join(PATH_IN, \"characters_old_annotations.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters # PV looks empty but it's not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb138724",
   "metadata": {},
   "source": [
    "### Generate bag of words\n",
    "A bag is a tupple of $(r,w)$, where $r$ is of {agent verb, patient verb, attribute} and $w$ is the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bags_of_words(characters_df: pd.DataFrame):\n",
    "    bags_of_words = []\n",
    "\n",
    "    for _, row in tqdm(characters_df.iterrows()):\n",
    "        movie_id = row['movie_id']\n",
    "        character_name = row['character']\n",
    "\n",
    "        av = row['AV'] if isinstance(row['AV'], list) else []\n",
    "        pv = row['PV'] if isinstance(row['PV'], list) else []\n",
    "        att = row['Att'] if isinstance(row['Att'], list) else []\n",
    "\n",
    "        for verb in av:\n",
    "            bags_of_words.append((movie_id, character_name, 'agent_verb', verb))\n",
    "\n",
    "        for verb in pv:\n",
    "            bags_of_words.append((movie_id, character_name, 'patient_verb', verb))\n",
    "\n",
    "        for attribute in att:\n",
    "            bags_of_words.append((movie_id, character_name, 'attribute', attribute))\n",
    "\n",
    "    return bags_of_words\n",
    "\n",
    "bags_of_words = generate_bags_of_words(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_df = pd.DataFrame(bags_of_words, columns=['movie_id', 'character_name', 'type', 'word'])\n",
    "bags_df.to_parquet(os.path.join(PATH_IN, \"bags_old_annotations.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214f6db",
   "metadata": {},
   "source": [
    "### From tupples to topics using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb930aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading bags of words\n",
    "bags_fname = os.path.join(PATH_IN,'bags_old_annotations.parquet')\n",
    "bags_df = pd.read_parquet(bags_fname)\n",
    "bags_of_words=bags_df.values\n",
    "\n",
    "bags_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb705da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently global character version (doesn't make the distinction of the same character in different movies)\n",
    "character_docs = defaultdict(list)\n",
    "for _, character, _, word in bags_of_words:\n",
    "    character_docs[character].append(word)\n",
    "    \n",
    "for character in character_docs:\n",
    "    character_docs[character] = \" \".join(character_docs[character])\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(character_docs.values())\n",
    "\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, verbose=2, max_iter=10, random_state = 0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lda, os.path.join(PATH_IN, \"lda_model_old_annotations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = joblib.load(os.path.join(PATH_IN,'lda_model_old_annotations.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff637bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic {topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "n_topic_words = 10\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), n_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a72c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_topic = lda.transform(X)\n",
    "character_names = list(character_docs.keys())\n",
    "\n",
    "character_classification=[]\n",
    "\n",
    "for i, topic_dist in enumerate(character_topic):\n",
    "    topic_most_prob = topic_dist.argmax()\n",
    "    character_classification.append((character_names[i], topic_most_prob, topic_dist))\n",
    "\n",
    "character_classification_df=pd.DataFrame(character_classification,columns=['character_name', 'topic', 'topic_dist'])\n",
    "character_classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_classification_df.to_parquet(os.path.join(PATH_IN, \"character_classification_old_annotations.parquet\"), compression= \"brotli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa86635",
   "metadata": {},
   "outputs": [],
   "source": [
    "character='Batman'\n",
    "\n",
    "character_topics = character_classification_df[character_classification_df['character_name'] == character]['topic_dist'].iloc[0]\n",
    "\n",
    "topics = range(len(character_topics))\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topics, batman_topics)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.title(f'Topic Distribution for {character}')\n",
    "plt.yscale('log')  \n",
    "plt.xticks(topics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = character_classification_df['topic'].value_counts()\n",
    "\n",
    "topic_counts = topic_counts.sort_index()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(topic_counts.index, topic_counts.values)\n",
    "\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Characters')\n",
    "plt.title('Distribution of Characters Across Topics')\n",
    "plt.xticks(topic_counts.index)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
