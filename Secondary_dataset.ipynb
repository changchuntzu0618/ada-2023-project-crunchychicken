{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2ea0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mutliple outputs in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# cell width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a85e0",
   "metadata": {},
   "source": [
    "# XML Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a00711",
   "metadata": {},
   "source": [
    "The plot summaries, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n",
    "\n",
    "[Paper](https://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf)\n",
    "\n",
    "[Dependency glossary](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7c7fa",
   "metadata": {},
   "source": [
    "### Imports and data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7313da6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42306"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['10000053.xml.gz',\n",
       " '10002175.xml.gz',\n",
       " '10002779.xml.gz',\n",
       " '10003264.xml.gz',\n",
       " '10004055.xml.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "PATH_IN = './XML_Dataset/'\n",
    "\n",
    "xml_gz_files = [f for f in os.listdir(PATH_IN) if f.endswith('.xml.gz')]\n",
    "len(xml_gz_files)\n",
    "xml_gz_files[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855d325",
   "metadata": {},
   "source": [
    "### XML file structure\n",
    "```\n",
    "sentences\n",
    "│ sentence id\n",
    "│ │ tokens\n",
    "│ │ │ token id\n",
    "│ │ │ │ word\n",
    "│ │ │ │ lemma\n",
    "│ │ │ │ char offset begin\n",
    "│ │ │ │ char offset end\n",
    "│ │ │ │ POS\n",
    "│ │ │ │ NER\n",
    "│ │ parse\n",
    "│ │ basic-dependencies\n",
    "│ │ │ dep\n",
    "│ │ │ │ governor\n",
    "│ │ │ │ dependent\n",
    "│ │ collapsed-dependencies\n",
    "│ │ │ dep\n",
    "│ │ │ │ governor\n",
    "│ │ │ │ dependent\n",
    "│ │ collapsed-ccprocessed-dependencies\n",
    "│ │ │ dep\n",
    "│ │ │ │ governor\n",
    "│ │ │ │ dependent\n",
    "```\n",
    "\n",
    "We will create three dataframes:\n",
    "- tokens: for the token data\n",
    "- parse: for the parse data\n",
    "- dependecies: for the dependencies data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32d120",
   "metadata": {},
   "source": [
    "### Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6575d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment to parse the 42k files\n",
    "# uncomment to only parse the first file, for dev purposes\n",
    "# xml_gz_files = [xml_gz_files[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "parses_data = []\n",
    "tokens_data = []\n",
    "dependencies_data = []\n",
    "\n",
    "for file_name in xml_gz_files:\n",
    "    movie_id = file_name.replace('.xml.gz', '')\n",
    "    file_path = os.path.join(PATH_IN, file_name)\n",
    "    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        xml_data = f.read()\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.get('id')\n",
    "            if sentence_id is not None:\n",
    "                \n",
    "                # appending to df_parses\n",
    "                parse = sentence.find('parse').text if sentence.find('parse') is not None else 'N/A'\n",
    "                parses_data.append({\"movie_id\": movie_id, \"sentence_id\": sentence_id, \"parse\": parse})\n",
    "\n",
    "                # appending to df_tokens\n",
    "                for token in sentence.findall('.//tokens/token'):\n",
    "                    token_id = token.get('id')\n",
    "                    word = token.find('word').text\n",
    "                    lemma = token.find('lemma').text\n",
    "                    char_offset_begin = token.find('CharacterOffsetBegin').text\n",
    "                    char_offset_end = token.find('CharacterOffsetEnd').text\n",
    "                    pos = token.find('POS').text\n",
    "                    ner = token.find('NER').text\n",
    "                    \n",
    "                    tokens_data.append({\n",
    "                        \"movie_id\": movie_id,\n",
    "                        \"sentence_id\": sentence_id,\n",
    "                        \"token_id\": token_id,\n",
    "                        \"word\": word,\n",
    "                        \"lemma\": lemma,\n",
    "                        \"COB\": char_offset_begin,\n",
    "                        \"COE\": char_offset_end,\n",
    "                        \"POS\": pos,\n",
    "                        \"NER\": ner,\n",
    "                    })\n",
    "\n",
    "                # appending to df_dependencies\n",
    "                for dep_class, dep_xpath in [(\"basic\", \"basic-dependencies\"),\n",
    "                                             (\"collapsed\", \"collapsed-dependencies\"),\n",
    "                                             (\"collapsed-ccprocessed\", \"collapsed-ccprocessed-dependencies\")]:\n",
    "                    for dep in sentence.findall(f'.//{dep_xpath}/dep'):\n",
    "                        dep_type = dep.get('type')\n",
    "                        governor_idx = dep.find('governor').get('idx')\n",
    "                        governor_text = dep.find('governor').text\n",
    "                        dependent_idx = dep.find('dependent').get('idx')\n",
    "                        dependent_text = dep.find('dependent').text\n",
    "                        dependencies_data.append({\n",
    "                            \"movie_id\": movie_id,\n",
    "                            \"sentence_id\": sentence_id,\n",
    "                            \"dependency_class\": dep_class,\n",
    "                            \"dependency_type\": dep_type,\n",
    "                            \"governor_id\": governor_idx,\n",
    "                            \"governor_word\": governor_text,\n",
    "                            \"dependent_id\": dependent_idx,\n",
    "                            \"dependent_word\": dependent_text,\n",
    "                        })\n",
    "\n",
    "tokens_df = pd.DataFrame(tokens_data)\n",
    "dependencies_df = pd.DataFrame(dependencies_data)\n",
    "parses_df = pd.DataFrame(parses_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03401c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df[\"movie_id\"] = tokens_df[\"movie_id\"].astype(\"int64\")\n",
    "tokens_df[\"sentence_id\"] = tokens_df[\"sentence_id\"].astype(\"int64\")\n",
    "tokens_df[\"token_id\"] = tokens_df[\"token_id\"].astype(\"int64\")\n",
    "tokens_df[\"COB\"] = tokens_df[\"COB\"].astype(\"int64\")\n",
    "tokens_df[\"COE\"] = tokens_df[\"COE\"].astype(\"int64\")\n",
    "tokens_df[\"word\"] = tokens_df[\"word\"].astype(\"string\")\n",
    "tokens_df[\"lemma\"] = tokens_df[\"lemma\"].astype(\"string\")\n",
    "tokens_df[\"POS\"] = tokens_df[\"POS\"].astype(\"string\")\n",
    "tokens_df[\"NER\"] = tokens_df[\"NER\"].astype(\"string\")\n",
    "tokens_df\n",
    "tokens_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_df[\"movie_id\"] = dependencies_df[\"movie_id\"].astype(\"int64\")\n",
    "dependencies_df[\"sentence_id\"] = dependencies_df[\"sentence_id\"].astype(\"int64\")\n",
    "dependencies_df[\"governor_id\"] = dependencies_df[\"governor_id\"].astype(\"int64\")\n",
    "dependencies_df[\"dependent_id\"] = dependencies_df[\"dependent_id\"].astype(\"int64\")\n",
    "dependencies_df[\"dependency_class\"] = dependencies_df[\"dependency_class\"].astype(\"string\")\n",
    "dependencies_df[\"dependency_type\"] = dependencies_df[\"dependency_type\"].astype(\"string\")\n",
    "dependencies_df[\"governor_word\"] = dependencies_df[\"governor_word\"].astype(\"string\")\n",
    "dependencies_df[\"dependent_word\"] = dependencies_df[\"dependent_word\"].astype(\"string\")\n",
    "dependencies_df\n",
    "dependencies_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parses_df[\"movie_id\"] = parses_df[\"movie_id\"].astype(\"int64\")\n",
    "parses_df[\"sentence_id\"] = parses_df[\"sentence_id\"].astype(\"int64\")\n",
    "parses_df[\"parse\"] = parses_df[\"parse\"].astype(\"string\")\n",
    "parses_df\n",
    "parses_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1064896",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.to_csv('tokens.csv', index=False)\n",
    "dependencies_df.to_csv('dependencies.csv', index=False)\n",
    "parses_df.to_csv('parses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2a118",
   "metadata": {},
   "source": [
    "### Old printing code (will be removed in next commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in xml_gz_files:\n",
    "    print(file_name)\n",
    "    file_path = os.path.join(PATH_IN, file_name)\n",
    "    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        xml_data = f.read()\n",
    "        \n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.get('id')\n",
    "            if sentence_id is not None:\n",
    "                print(f\"Sentence ID: {sentence_id}\")\n",
    "\n",
    "                # Process each token in the sentence\n",
    "                for token in sentence.findall('.//tokens/token'):\n",
    "                    token_id = token.get('id')\n",
    "                    word = token.find('word').text\n",
    "                    lemma = token.find('lemma').text\n",
    "                    char_offset_begin = token.find('CharacterOffsetBegin').text\n",
    "                    char_offset_end = token.find('CharacterOffsetEnd').text\n",
    "                    pos = token.find('POS').text\n",
    "                    ner = token.find('NER').text\n",
    "\n",
    "                    print(f\"Token ID: {token_id}, Word: {word}, Lemma: {lemma}, COB: {char_offset_begin}, COE_ {char_offset_end} POS: {pos}, NER: {ner}\")\n",
    "\n",
    "                # Extract parse information for each sentence\n",
    "                parse = sentence.find('parse').text if sentence.find('parse') is not None else 'N/A'\n",
    "                print(f\"Parse: {parse}\")\n",
    "\n",
    "                # Process dependencies for each sentence\n",
    "                basic_deps = sentence.find('basic-dependencies')\n",
    "                if basic_deps is not None:\n",
    "                    for dep in basic_deps.findall('dep'):\n",
    "                        dep_type = dep.get('type')\n",
    "                        governor_idx = dep.find('governor').get('idx')\n",
    "                        governor_text = dep.find('governor').text\n",
    "                        dependent_idx = dep.find('dependent').get('idx')\n",
    "                        dependent_text = dep.find('dependent').text\n",
    "                        print(f\"Basic Dependency Type: {dep_type}, Governor idx={governor_idx}: {governor_text}, Dependent idx={dependent_idx}: {dependent_text}\")\n",
    "\n",
    "                # Process collapsed dependencies\n",
    "                collapsed_deps = sentence.find('collapsed-dependencies')\n",
    "                if collapsed_deps is not None:\n",
    "                    for dep in collapsed_deps.findall('dep'):\n",
    "                        dep_type = dep.get('type')\n",
    "                        governor_idx = dep.find('governor').get('idx')\n",
    "                        governor_text = dep.find('governor').text\n",
    "                        dependent_idx = dep.find('dependent').get('idx')\n",
    "                        dependent_text = dep.find('dependent').text\n",
    "                        print(f\"Collapsed Dependency Type: {dep_type}, Governor idx={governor_idx}: {governor_text}, Dependent idx={dependent_idx}: {dependent_text}\")\n",
    "\n",
    "                # Process collapsed-ccprocessed dependencies\n",
    "                ccprocessed_deps = sentence.find('collapsed-ccprocessed-dependencies')\n",
    "                if ccprocessed_deps is not None:\n",
    "                    for dep in ccprocessed_deps.findall('dep'):\n",
    "                        dep_type = dep.get('type')\n",
    "                        governor_idx = dep.find('governor').get('idx')\n",
    "                        governor_text = dep.find('governor').text\n",
    "                        dependent_idx = dep.find('dependent').get('idx')\n",
    "                        dependent_text = dep.find('dependent').text\n",
    "                        print(f\"Collapsed-CCProcessed Dependency Type: {dep_type}, Governor idx={governor_idx}: {governor_text}, Dependent idx={dependent_idx}: {dependent_text}\")\n",
    "\n",
    "                print(\"\\n\")  # Print a newline to separate sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae5b2c",
   "metadata": {},
   "source": [
    "# Thinking on how to save the data for all the files\n",
    "TODO:\n",
    "- Finish XML for all the files\n",
    "- check how to use wiki and freebase id with wikidata\n",
    "- check for a scrapper on wikidata + IMDb or other\n",
    "- Create a notebook with all the saves, probably move the single file notebook to an exploration branch\n",
    "- create a drawboard with all the files and their columns to clearly see the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddadb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
